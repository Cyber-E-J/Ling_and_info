{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "872722794e74995476ee2a693a88e5ba",
     "grade": false,
     "grade_id": "cell-05fb407e20c068e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 5: \"No one can be told what the Ranktrix is. You have to see it for yourself.\"\n",
    "\n",
    "## Â© Cristian Danescu-Niculescu-Mizil 2023\n",
    "\n",
    "## CS/INFO 4300 Language and Information\n",
    "\n",
    "### Due by 11:59PM on Friday March 03, 2023\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "For code completion tasks, just type your code after the comment marking the place.  For questions, use as many notebook cells as needed to compute intermediate stuff.\n",
    "\n",
    "You are strongly encouraged to write sensible **test cases** for your code.\n",
    "\n",
    "This is an **individual** assignment.\n",
    "\n",
    "If you use any outside sources (e.g. research papers, StackOverflow) please list your sources.\n",
    "\n",
    "In this assignment we will explore evaluation of an information retrevial system where both queries and results are movies. Ever wanted to know what the most similar movie to \"The Matrix\" is, in terms of language? Now is your chance! You take the blue pill - the story ends, you wake up in your dorm on west campus and believe whatever you want to believe. You take the red pill - you stay in CS/INFO 4300 and we show you how deep the rabbit-hole goes.\n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "* All cells that contain the blocks that read `# YOUR CODE HERE` are editable and are to be completed to ensure you pass the test-cases. Make sure to write your code where indicated.\n",
    "\n",
    "* All cells that read `YOUR ANSWER HERE` are free-response cells that are editable and are to be completed.\n",
    "\n",
    "* Please delete raise `NotImplementedError()` after filling in the function code. It is only meant to be a temporary placeholder\n",
    "\n",
    "* You may use any number of notebook cells to explore the data and test out your functions, although you will only be graded on the solution itself.\n",
    "\n",
    "* You are unable to modify the read-only cells.\n",
    "\n",
    "* You should also use Markdown cells to explain your code and discuss your results when necessary.\n",
    "Instructions can be found [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "* All floating point values should be printed with **2 decimal places** precision. You can do so using the built-in round function.\n",
    "\n",
    "* **Never delete any code / free response and autograder test cell.**\n",
    "\n",
    "* Do not delete the cells for optional questions, even if you do not choose to answer them.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "This project aims to help you get comfortable working with the following tools / technologies / concepts:\n",
    "\n",
    "* TF-IDF vectorization using sklearn\n",
    "* Similarity matrices\n",
    "* Precision & Recall\n",
    "* Cosine similarity vs Jaccard similarity\n",
    "* Rocchio Algorithm\n",
    "\n",
    "**Grading**\n",
    "\n",
    "For code-completion questions you will be graded on passing the public test cases we have included, as well as any hidden test cases that we have supplemented to ensure that your logic is correct.\n",
    "\n",
    "For free-response questions you will be manually graded on the quality of your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "558664013aacfecb74ff70f3962fb2c3",
     "grade": false,
     "grade_id": "cell-85960141460df3c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "In this assignment, we will be building a system that allows you to query for movies similar to a given movie. Unlike before, queries and information retrieved have the same type -- i.e. movies are *both* queries and results. To accomplish this task, we will utilize a dataset of movies and their transcripts. We will begin by using the language contained in the transcripts to do basic queries. We will continue to use the vector space model, encoding \"documents\" (here, a document is a movie script) as tf-idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "549462241d79ceaf535efdc9cd232e18",
     "grade": false,
     "grade_id": "cell-409acf73e9b10d87",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import json\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba8a0388b2a197569416528c72f70425",
     "grade": false,
     "grade_id": "cell-12fdde1aa8750596",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Using sklearn to get tf-idf vectors\n",
    "On the last assignment, we used an inverted index to quickly compute queries by taking advantage of the sparsity of tf-idf vectors in our vector space. However, the dataset we are considering for this assignment is small enough such that we can use explicit vectors, rather than an inverted index, to compute cosine similarities. We also will not be implementing tf-idf by hand -- we will be using an existing implementation from the library [sklearn,](http://scikit-learn.org/stable/) which provides a lot of good implementations of machine learning algorithms in python. We will be making heavy use of this powerful library later in the semester, so using it to extract tfidf features is a good starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f1970974d230c84ae14b2424042b3b",
     "grade": false,
     "grade_id": "cell-6563bc55c31b7968",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 617 movie transcripts\n",
      "Each movie transcript is a dictionary with the following keys...\n",
      "dict_keys(['movie_name', 'movie_id', 'categories', 'script'])\n",
      "The index of \"spare me\" is 7\n"
     ]
    }
   ],
   "source": [
    "with open(\"movie_scripts_data.json\") as f:\n",
    "    data = json.loads(f.readlines()[0])\n",
    "num_movies = len(data)\n",
    "print(\"Loaded {} movie transcripts\".format(num_movies))\n",
    "print(\"Each movie transcript is a dictionary with the following keys...\")\n",
    "print(data[0].keys())\n",
    "\n",
    "# Here, we will assign an index for each movie_id. This index will help us access data in numpy matrices.\n",
    "movie_id_to_index = {movie_id:index for index, movie_id in enumerate([d['movie_id'] for d in data])}\n",
    "\n",
    "# We will also need a dictionary maping movie names to movie ids\n",
    "movie_name_to_id = {name:mid for name, mid in zip([d['movie_name'] for d in data],\n",
    "                                                     [d['movie_id'] for d in data])}\n",
    "movie_id_to_name = {v:k for k,v in movie_name_to_id.items()}\n",
    "\n",
    "# and because it might be useful...\n",
    "movie_name_to_index = {name:movie_id_to_index[movie_name_to_id[name]] for name in [d['movie_name'] for d in data]}\n",
    "movie_index_to_name = {v:k for k,v in movie_name_to_index.items()}\n",
    "\n",
    "movie_names = [name for name in [d['movie_name'] for d in data]]\n",
    "\n",
    "print(\"The index of \\\"{}\\\" is {}\".format(data[7]['movie_name'], movie_id_to_index[data[7]['movie_id']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8122747da17e52a4f657d5607e14a321",
     "grade": false,
     "grade_id": "cell-081a2217d72930ff",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We can see that each movie is assigned an \"index\" (from 0 to 616). These will correspond to the rows of a document-by-tfidf score matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6ebed4f269748f249b051b2266ab63f",
     "grade": false,
     "grade_id": "cell-414c5e6624055e75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1 (Code Completion): TFIDF Vectorizer\n",
    "Read up on sklearn's [tfidf-vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). This function takes in a list of documents and some parameters related to parsing and outputs a document-by-vocabulary matrix, where entry i,j corresponds to the tfidf score of word j in document i. \n",
    "\n",
    "Your first job is to make a TfidfVectorizer object that includes the following preprocessing properties (look at the documentation to learn how to achieve these): \n",
    "- It only considers words that appear in _at least_ ten documents, but in _no more_ than 80% of all documents.\n",
    "- It computes a maximum of 5000 features, and detects and filters out stopwords in English.\n",
    "- It should normalize all tfidf vectors to have an l2 norm of 1.\n",
    "\n",
    "Once you've made this object, call its `fit_transform()` function on the list of *scripts* (not titles) in data. This should produce a numpy matrix whose shape is the number of documents by the number of words you're considering (which has a maximum of 5000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f8e5ed7dca8e416a8f30c691410cb8c",
     "grade": false,
     "grade_id": "build_doc_by_vocab",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "n_feats = 5000\n",
    "doc_by_vocab = np.empty([len(data), n_feats])\n",
    "\n",
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    \"\"\"Returns a TfidfVectorizer object with the above preprocessing properties.\n",
    "    \n",
    "    Note: This function may log a deprecation warning. This is normal, and you\n",
    "    can simply ignore it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_features : int\n",
    "        Corresponds to 'max_features' parameter of the sklearn TfidfVectorizer \n",
    "        constructer.\n",
    "    stop_words : str\n",
    "        Corresponds to 'stop_words' parameter of the sklearn TfidfVectorizer constructer. \n",
    "    max_df : float\n",
    "        Corresponds to 'max_df' parameter of the sklearn TfidfVectorizer constructer. \n",
    "    min_df : float\n",
    "        Corresponds to 'min_df' parameter of the sklearn TfidfVectorizer constructer. \n",
    "    norm : str\n",
    "        Corresponds to 'norm' parameter of the sklearn TfidfVectorizer constructer. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TfidfVectorizer\n",
    "        A TfidfVectorizer object with the given parameters as its preprocessing properties.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c864ba21d6163ac4daee3cee331c8aa",
     "grade": false,
     "grade_id": "cell-86320f51d83cfd9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "doc_by_vocab = tfidf_vec.fit_transform([d['script'] for d in data]).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8063bf0b026ef16ac0b031af81dd29dd",
     "grade": true,
     "grade_id": "build_doc_by_vocab_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_vectorizer returns the correct output\"\"\"\n",
    "assert type(doc_by_vocab) == np.ndarray\n",
    "assert type(tfidf_vec) == TfidfVectorizer\n",
    "assert sum(doc_by_vocab[2,:]) < 20\n",
    "assert doc_by_vocab.shape == (617, 5000)\n",
    "assert 'zoo' in index_to_vocab.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "836aff0bda837eb8517486cd2504da1a",
     "grade": false,
     "grade_id": "cell-44503d2b6b03160a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2 (Code Completion): Cosine Similarity\n",
    "You will implement the function below which takes in the names of two movies, a term-document matrix of movie transcripts, and a dictionary that maps movie names to the corresponding row index in the term-document matrix.\n",
    "\n",
    "Remember that cosine similarity is defined as:\n",
    "\n",
    "$$ cossim(\\vec{q}, \\vec{d_j}) = \\frac{\\vec{q} \\cdot \\vec{d_j}}{\\|\\vec{q}\\| \\cdot \\|\\vec{d_j}\\|}$$\n",
    "\n",
    "\n",
    "Hint: As always, make good use of numpy to make your implementation efficient, as this method will be called in later questions. However, note that you cannot import off-the-shelf implementation of cosine similarity for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1b73baf1d838505841a62f69cbf82f1",
     "grade": false,
     "grade_id": "get_sim",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_sim(mov1, mov2, input_doc_mat, input_movie_name_to_index):\n",
    "    \"\"\"Returns a float giving the cosine similarity of \n",
    "       the two movie transcripts.\n",
    "    \n",
    "    Params: {mov1 (str): Name of the first movie.\n",
    "             mov2 (str): Name of the second movie.\n",
    "             input_doc_mat (numpy.ndarray): Term-document matrix of movie transcripts, where \n",
    "                    each row represents a document (movie transcript) and each column represents a term.\n",
    "             movie_name_to_index (dict): Dictionary that maps movie names to the corresponding row index \n",
    "                    in the term-document matrix.}\n",
    "    Returns: Float (Cosine similarity of the two movie transcripts.)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7440ea8da380431e2367bad91e1cdeeb",
     "grade": true,
     "grade_id": "get_sim_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that get_sim returns the correct output\"\"\"\n",
    "print(\"Similarity: Star Wars vs. Jurassic Park\")\n",
    "print(\"======\")\n",
    "test1 = get_sim('star wars', 'jurassic park', doc_by_vocab, movie_name_to_index)\n",
    "print(test1)\n",
    "assert test1 < 0.07 and test1 > 0.06\n",
    "print(\"\")\n",
    "\n",
    "print(\"Similarity: Star Wars vs. Star Trek: Generations\")\n",
    "print(\"======\")\n",
    "test2 = get_sim('star wars', 'star trek: generations', doc_by_vocab, movie_name_to_index)\n",
    "print(test2)\n",
    "assert test2 < 0.25 and test2 > 0.20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e2a7653775d77c9ce70a978defbe19",
     "grade": false,
     "grade_id": "cell-f4005b2d38712fdb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3 (Code Completion): Term Similarity\n",
    "Complete the function `top_terms`, which takes in the list of movie names, and returns the top matching tfidf terms from these movie transcripts. Consider doing an element-wise product of the movies' tfidf vectors. If all vectors have a high value for a particular term, then it is contributing to the cosine similarity for each (e.g. if both \"star wars\" and \"jurassic park\" both had high tfidf weights for \"fight,\" then this word would likely be a top_term for the two). After performing the element-wise product, find the indices that produce the highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f487ac77df59e5c207b4832e60c0d3f5",
     "grade": false,
     "grade_id": "top_terms",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def top_terms(movs, input_doc_mat, index_to_vocab, movie_name_to_index, top_k=10):\n",
    "    \"\"\"Returns a list of the top k similar terms (in order) between the\n",
    "        inputted movie transcripts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    movs : str list (Length >= 2)\n",
    "        List of movie names \n",
    "    input_doc_mat : np.ndarray\n",
    "        The term document matrix of the movie transcripts. input_doc_mat[i][j] is the tfidf\n",
    "        of the movie i for the word j.\n",
    "    index_to_vocab : dict\n",
    "         A dictionary linking the index of a word (Key: int) to the actual word (Value: str). \n",
    "         Ex: {0: 'word_0', 1: 'word_1', .......}\n",
    "    movie_name_to_index : dict\n",
    "         A dictionary linking the movie name (Key: str) to the movie index (Value: int). \n",
    "         Ex: {'movie_0': 0, 'movie_1': 1, .......}\n",
    "    top_k : int\n",
    "        The k in the top k similar words to be returned. Ex: If top_k = 8, return top 8 similar words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the top k similar terms (in order) between the inputted movie transcripts\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b399a3dd3dc4809242f6f2a93379c8b2",
     "grade": true,
     "grade_id": "top_terms_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that get_sim returns the correct output\"\"\"\n",
    "print(\"Top ten terms between: Star Wars and Jurassic Park\")\n",
    "print(\"======\")\n",
    "term_test_1 = top_terms(['star wars', 'jurassic park'], doc_by_vocab, index_to_vocab, movie_name_to_index)\n",
    "for term in term_test_1:\n",
    "    print(term)\n",
    "assert 'force' in term_test_1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Top ten terms between: Star Wars and Star Trek: Generations\")\n",
    "term_test_2 = top_terms(['star wars', 'star trek: generations'], doc_by_vocab,index_to_vocab, movie_name_to_index)\n",
    "assert 'star' in term_test_2\n",
    "print(\"======\")\n",
    "for term in term_test_2:\n",
    "    print(term)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Top ten terms between: Star Wars and Star Trek: Generations and Jurassic Park\")\n",
    "term_test_3 = top_terms(['star wars', 'star trek: generations', 'jurassic park'], doc_by_vocab,index_to_vocab, movie_name_to_index)\n",
    "assert 'attack' in term_test_3\n",
    "print(\"======\")\n",
    "for term in term_test_3:\n",
    "    print(term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3b (Free Response): Common terms interpretation\n",
    "\n",
    "\n",
    "In the cell below, state what you noticed about the common terms shared by the three movies in the test case ('star wars', 'star trek: generations', 'jurassic park'). Did the common terms represent some common themes, categories, or genres shared by these movies? \n",
    "<br>\n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05f390f4fc642c361a28365eaf32a19f",
     "grade": false,
     "grade_id": "cell-f6d043750f734cf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4 (Code Completion): Similar Transcripts, Similar Movies w/ Cosine Sim\n",
    "\n",
    "Given your `get_sim` function, you can now compute how similar movies are to one another! Here, we will first precompute the similarity between every possible pair of movies, and store it in a movies-by-movies matrix. Given this matrix, for a given movie, it is possible to produce a ranking of how similar all other movies are to the given movie. For instance, we will see what movies are the most similar and dissimilar to \"star wars\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bac8503fcd747bb79381942213fbe42",
     "grade": false,
     "grade_id": "build_movie_sims_cos",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_movie_sims_cos(n_mov, movie_index_to_name, input_doc_mat, movie_name_to_index, input_get_sim_method):\n",
    "    \"\"\"Returns a movie_sims matrix of size (num_movies,num_movies) where for (i,j):\n",
    "        [i,j] should be the cosine similarity between the movie with index i and the movie with index j\n",
    "        \n",
    "    Note: You should set values on the diagonal to 1\n",
    "    to indicate that all movies are trivially perfectly similar to themselves.\n",
    "    \n",
    "    Params: {n_mov: Integer, the number of movies\n",
    "             movie_index_to_name: Dictionary, a dictionary that maps movie index to name\n",
    "             input_doc_mat: Numpy Array, a numpy array that represents the document-term matrix\n",
    "             movie_name_to_index: Dictionary, a dictionary that maps movie names to index\n",
    "             input_get_sim_method: Function, a function to compute cosine similarity}\n",
    "    Returns: Numpy Array \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d6cbf0b3d16933f76d44c5d698f4355",
     "grade": false,
     "grade_id": "cell-ede81a4209974a0c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "movie_sims_cos = build_movie_sims_cos(num_movies, movie_index_to_name, doc_by_vocab, movie_name_to_index, get_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19b823c8445e3d61f8006203ea14786d",
     "grade": true,
     "grade_id": "movie_sims_cos_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_movie_sims_cos returns the correct output\"\"\"\n",
    "assert type(movie_sims_cos) == np.ndarray\n",
    "assert movie_sims_cos.shape == (617,617)\n",
    "assert movie_sims_cos[15,15] == 1.0\n",
    "assert sum(movie_sims_cos[:,5]) > 40\n",
    "test_star_trek = movie_sims_cos[movie_name_to_index[\"star trek iii: the search for spock\"]][movie_name_to_index[\"star trek: the wrath of khan\"]]\n",
    "assert test_star_trek > 0.6 and test_star_trek < 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27215e07346fe49743430f78cdfb5974",
     "grade": false,
     "grade_id": "cell-ee2e54a7329b3a3d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4b (Code Completion): Similar Transcripts, Similar Movies w/ Jaccard Sim\n",
    "\n",
    "For a baseline comparison, we will also consider the similarity between movies in terms of categories. Specifically, you'll notice that each movie is associated with a list of categories. One could ignore the transcripts *entirely* and say that the similarity between any two movies is the jaccard similarity of their category sets. While this is a very rough way to measure similarity, we will use it as a baseline for comparison with the linguistic methods.\n",
    "\n",
    "Hint: The keys of the dictionary are printed out in the same cell the data is loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88546fb9e2b0918e9ec7489940485ace",
     "grade": false,
     "grade_id": "build_movie_sims_jac",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_movie_sims_jac(n_mov, input_data):\n",
    "    \"\"\"Returns a movie_sims_jac matrix of size (num_movies,num_movies) where for (i,j) :\n",
    "        [i,j] should be the jaccard similarity between the category sets for movies i and j\n",
    "        such that movie_sims_jac[i,j] = movie_sims_jac[j,i]. \n",
    "        \n",
    "    Note: \n",
    "        Movies sometimes contain *duplicate* categories! You should only count a category once\n",
    "        \n",
    "        A movie should have a jaccard similarity of 1.0 with itself.\n",
    "    \n",
    "    Params: {n_mov: Integer, the number of movies,\n",
    "            input_data: List<Dictionary>, a list of dictionaries where each dictionary \n",
    "                     represents the movie_script_data including the script and the metadata of each movie script}\n",
    "    Returns: Numpy Array \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17b2c61fdec60225b044856be79c1fd6",
     "grade": false,
     "grade_id": "cell-9ffeab8de1a863dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "movie_sims_jac = build_movie_sims_jac(num_movies,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a3859f190dcfc8e34e178cc527da6c9",
     "grade": true,
     "grade_id": "movie_sims_jac_test",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_movie_sims_cos returns the correct output\"\"\"\n",
    "assert type(movie_sims_jac) == np.ndarray\n",
    "assert movie_sims_jac.shape == (617,617)\n",
    "assert sum(movie_sims_jac[:,5]) > 60 and sum(movie_sims_jac[:,5]) < 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b366e7bae9c50af0793d1f0d14189b4",
     "grade": false,
     "grade_id": "cell-4a540d60107ce945",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4c (Free Response): Similar Transcripts, Similar Movies\n",
    "\n",
    "Using the `movie_sims_cos` and `movie_sims_jac` matrices you computed, we are now going to compare cosine vs. jaccard similarity by printing the 10 most similar (ignoring itself) and 10 most dissimilar movies to 'star wars'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60b5c59119a7eec8e0fffa2321b9d2a9",
     "grade": false,
     "grade_id": "cell-19eda7bcc6d614bb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ranked_movies(mov, matrix):\n",
    "    \"\"\"\n",
    "    Return sorted rankings (most to least similar) of movies as \n",
    "    a list of two-element tuples, where the first element is the \n",
    "    movie name and the second element is the similarity score\n",
    "    \n",
    "    Params: {mov: String,\n",
    "             matrix: np.ndarray}\n",
    "    Returns: List<Tuple>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get movie index from movie name\n",
    "    mov_idx = movie_name_to_index[mov]\n",
    "    \n",
    "    # Get list of similarity scores for movie\n",
    "    score_lst = matrix[mov_idx]\n",
    "    mov_score_lst = [(movie_index_to_name[i], s) for i,s in enumerate(score_lst)]\n",
    "    \n",
    "    # Do not account for movie itself in ranking\n",
    "    mov_score_lst = mov_score_lst[:mov_idx] + mov_score_lst[mov_idx+1:]\n",
    "    \n",
    "    # Sort rankings by score\n",
    "    mov_score_lst = sorted(mov_score_lst, key=lambda x: -x[1])\n",
    "    \n",
    "    return mov_score_lst\n",
    "\n",
    "\n",
    "def print_top(mov, matrix, sim_type, k=10):\n",
    "    \"\"\"\n",
    "    Print the k most and least similar movies to 'star wars'\n",
    "    \n",
    "    Params: {mov: String,\n",
    "             matrix: np.ndarray,\n",
    "             sim_type: String,\n",
    "             k: Integer}\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    mov_score_lst = get_ranked_movies(mov, matrix)\n",
    "    \n",
    "    print(\"Top {} most similar movies to {} [{}]\".format(k, 'star wars', sim_type))\n",
    "    print(\"======\")\n",
    "    for (mov, score) in mov_score_lst[:k]:\n",
    "        print(\"%.3f %s\" % (score, mov))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print(\"Top {} least similar movies to {} [{}]\".format(k, 'star wars', sim_type))\n",
    "    print(\"======\")\n",
    "    for (mov, score) in mov_score_lst[-k:][::-1]:\n",
    "        print(\"%.3f %s\" % (score, mov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96b7201032484f8c045b80734ba3b587",
     "grade": false,
     "grade_id": "cell-74bce35a04df44cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print_top('star wars', movie_sims_cos, 'cosine sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f7c748ee36d21a9a7447f022cd7e5ba",
     "grade": false,
     "grade_id": "cell-c81f7651cd4500b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print_top('star wars', movie_sims_jac, 'jaccard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "727bd3f9355e15034f38ae49cf55bf5c",
     "grade": false,
     "grade_id": "cell-25fa76331ae8813d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the cell below, analyze the most similar and least similar results above for both cosine similarity and jaccard similarity. Please comment on how well (or poorly) you think both of the similarity measures performed. Do these results make sense to you? Why are certain movies ranked more similar to Star Wars in one similarity measure than the other one?\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "492dc43965f52cf732cc12e3e6994dd8",
     "grade": true,
     "grade_id": "most_least_sim_movies_ans",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d8547781d026464d35b7493cf646ba4",
     "grade": false,
     "grade_id": "cell-40c993e89a549adb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b020e79daab508e799f1c32202354fa7",
     "grade": false,
     "grade_id": "cell-8f564d9e0d9278b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5 (Code Completion): Evaluating our rankings with precision-recall curves\n",
    "\n",
    "Given that we are able to produce a most-to-least similar ranking of all other movies given all other movies, we can now ask the question: \"How good are our rankings?\"\n",
    "\n",
    "For this part, we will be using the following (query, [related movie list]) pairs for you to evaluate against. We will treat these data as ground truth. More generally, you could imagine that these \"ground truths\" result from aggregated user feedback from a movie recomendation system like Netflix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1991802065e9d32a6f2f5ca7b6c5a7f",
     "grade": false,
     "grade_id": "cell-d0af2bf1d4f220ea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "queries = [('the matrix', ['the bourne supremacy',\n",
    "                           'being john malkovich',\n",
    "                           'smoke',\n",
    "                           'erin brockovich',\n",
    "                           'an officer and a gentleman',\n",
    "                           'smokin\\' aces',\n",
    "                           'pitch black',\n",
    "                           'out of sight',\n",
    "                           'clerks.',\n",
    "                           'xxx',\n",
    "                           'the x files',\n",
    "                          ]),\n",
    "           ('star wars',  ['star wars: the empire strikes back',\n",
    "                            'star wars: episode vi - return of the jedi',\n",
    "                            'indiana jones and the last crusade',\n",
    "                            'indiana jones and the temple of doom',\n",
    "                            'jurassic park',\n",
    "                            'the lost world: jurassic park',\n",
    "                            'jurassic park iii',\n",
    "                            'star trek v: the final frontier',\n",
    "                            'star trek: the motion picture',\n",
    "                            'star trek: first contact',\n",
    "                            'star trek vi: the undiscovered country',\n",
    "                            'star trek iv: the voyage home',\n",
    "                            'the majestic',\n",
    "                            'hannibal',\n",
    "                            'star trek: insurrection',\n",
    "                            'dr. strangelove or: how i learned to stop worrying and love the bomb'\n",
    "                          ]),\n",
    "          ('a nightmare on elm street', ['a nightmare on elm street part 2: freddy\\'s revenge',\n",
    "                                         'a nightmare on elm street: the dream child',\n",
    "                                         'cruel intentions',\n",
    "                                         'erin brockovich',\n",
    "                                         'hellraiser: hellseeker',\n",
    "                                         'little nicky',\n",
    "                                        ]),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "336eeb0838b4d3a0b7c869e4fb1d1b55",
     "grade": false,
     "grade_id": "cell-31a990e9921e58ca",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "To answer our question, we will now look at two ways of evaluating our rankings:\n",
    "1. Precision vs. Recall plots\n",
    "\n",
    "2. An evaluation statistic called Mean Average Precision\n",
    "\n",
    "To start off, **complete the `precision_recall` function below.**\n",
    "To recap, `precision @ K` measures the precision in the subset of documents that were retrieved _up to_ an index $K$, and `recall @ K` is defined similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5259a7ead11402825cab8e3f18f75924",
     "grade": false,
     "grade_id": "precision_recall",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def precision_recall(ranking_in, relevant):\n",
    "    \"\"\"\n",
    "    Returns lists of precision and recall at different k values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ranking_in : str list \n",
    "        List with sorted ranking of movies (movie names), starting with the most similar, and ending\n",
    "        with the least similar.\n",
    "    relevant : str list\n",
    "        List of movies (movie names) relevant to the original query\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (np.ndarray, np.ndarray)\n",
    "        Returns tuple such that tuple[0] is numpy array of precision at different k values and \n",
    "        tuple[1] is numpy array of recall at different k values. \n",
    "    \n",
    "        tuple[0] -> precision: numpy array of length equal to the length+1 of ranking_in, where \n",
    "        precision[k] = the precision@k. Leave precision[0] to be 0.\n",
    "        \n",
    "        tuple[1] -> recall: numpy array of length equal to the length+1 of ranking_in, where \n",
    "        recall[k] = the recall@k. Leave recall[0] to be 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5585f1568ed73ed95b5f05f7fc56bf5e",
     "grade": true,
     "grade_id": "precision_recall_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that precision_recall returns the correct output\"\"\"\n",
    "query, rel_movs = queries[1]\n",
    "ranked_movs = [m for m,_ in get_ranked_movies(query, movie_sims_cos)]\n",
    "precision, recall = precision_recall(ranked_movs, rel_movs)\n",
    "\n",
    "assert precision[0] == 0\n",
    "assert recall[0] == 0\n",
    "assert precision.shape == (617,)\n",
    "assert recall.shape == (617,)\n",
    "assert sum(precision) > 48 and sum(precision) < 54\n",
    "assert sum(recall) > 505 and sum(recall) < 511\n",
    "assert precision[300] > 0.04 and precision[300] < 0.05\n",
    "assert recall[300] > 0.8 and recall[300] < 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1d48a3c666a3f974b253c822662640f",
     "grade": false,
     "grade_id": "cell-ea997339be861abf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Plotting Precision-Recall Curves\n",
    "\n",
    "Below, we have provided the code that uses matplotlib to create a recall (x-axis) vs. precision (y-axis) plot, plotting each of the 3 ground truth queries as seperate lines on the plot. For each query, we consider all N-1 movies other than the query itself when computing the ranking. The label of each line is the ground truth query. \n",
    "\n",
    "It's worth noting that considering precision/recall curves like this on the query level is a bit odd. In general, given more supervised data, one would compute aggregate precision/recall curves over a large number of ground truths. In this way, each line would represent a different information retrieval algorithm's performance over the test set of queries. However, we will be somewhat unorthodox here and plot one curve per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7193a79d106c862dc4cc5ce948f8265a",
     "grade": false,
     "grade_id": "cell-9b33baf2399a497f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(matrix, xlim, ylim):\n",
    "    \"\"\"Plots the precision-recall curve given the similarity matrix\n",
    "    \n",
    "    Params: {matrix: np.ndarray,\n",
    "             xlim: List,\n",
    "             ylim: List}\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    for query, rel_movs in queries:\n",
    "        ranked_movs = [m for m,_ in get_ranked_movies(query, matrix)]\n",
    "        precision, recall = precision_recall(ranked_movs, rel_movs)\n",
    "        plt.plot(recall, precision)\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.legend([q[0] for q in queries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc81ac91b2a8dee50a0862c905eb1411",
     "grade": false,
     "grade_id": "cell-db5371070c5ffeac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5b (Free Response): Cosine Similarity Plot Analysis\n",
    "\n",
    "Run the code below to show the precision-recall curve for our three movie queries using cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a8ffa0a4ddd3c71496675f2e41af392",
     "grade": false,
     "grade_id": "cell-0c56de23d0985e7c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Cosine Similarity Plot\n",
    "plot_precision_recall(movie_sims_cos, [0,1.1], [0,1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7be7451b1942661fc2de41fa4c80f72e",
     "grade": false,
     "grade_id": "cell-ad9ada38bdd57133",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Looking at the cosine similarity plot above, make some observations about why you're seeing what you're seeing. What does the plot reveal about the performance of the cosine similarity method for the given queries? Include observations on what worked well, what didn't, and how you've arrived at these conclusions. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2871fdff96652e713bd2dead84b9458",
     "grade": true,
     "grade_id": "cos_sim_precision_recall_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "036cfd147c5527c73dc51a3eebd4c23f",
     "grade": false,
     "grade_id": "cell-f8fb289995e9b6b2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d51743e8abf41358a9bc7a8fc20846e7",
     "grade": false,
     "grade_id": "cell-bbd3cce37bf8d9d4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5c (Free Response): Jaccard Similarity Plot Analysis\n",
    "\n",
    "Run the code below to show the precision-recall curve for our three movie queries using jaccard similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1450cc4bb725dc1e468be421dee90c43",
     "grade": false,
     "grade_id": "cell-bf2fe6a0cfbd5303",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Jaccard Similarity Plot\n",
    "plot_precision_recall(movie_sims_jac, [0,1.1], [0,0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a370c5bc44e71b68489381b8dc26602",
     "grade": false,
     "grade_id": "cell-72aaf601ea602b3e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Looking at the jaccard similarity plot above,  make some observations about why you're seeing what you're seeing. What does the plot reveal about the performance of the jaccard similarity method for the given queries? Include observations on what worked well, what didn't, and how you've arrived at these conclusions. \n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a38b9f503263aef130792ee40cabff73",
     "grade": true,
     "grade_id": "jaccard_precision_recall_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b7759312a08df93610d59d55ca97962",
     "grade": false,
     "grade_id": "cell-750e4e70738c4fc0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a4044738ea5261ba5ee1d36f47392b6",
     "grade": false,
     "grade_id": "cell-cc3dc0d1770e67ea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5d (Free Response): Cosine Sim vs Jaccard Sim Plot Comparison\n",
    "\n",
    "Now that you've observed the plots for both cosine similarity and jaccard similarity, please answer the following two questions:\n",
    "\n",
    "1. Which system performs better, in general?\n",
    "2. Which query was the most problematic in each case?\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf0dc9000beec71c737bee0586339cf3",
     "grade": true,
     "grade_id": "cos_sim_jaccard_plot_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4331352beab43da0c14cadfd4ee4b1cf",
     "grade": false,
     "grade_id": "cell-e00b96543d525387",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "856dbff088246de017680aadadc49b90",
     "grade": false,
     "grade_id": "cell-90e94cb41383c723",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5e (Code Completion): Calculating F-score Values\n",
    "\n",
    "Now that you have seen the precison-recall, we will calculate the F-score foreach of the queries and see if they reveal any new information. As a reminder, the formuala for calculating the F-score is given below.\n",
    "\n",
    "$$F-score = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$$ \n",
    "\n",
    "Note: If Precision or Recall is 0 we will just state that the fscore is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fda1671b4e0cefc8895d1d0549f372d",
     "grade": false,
     "grade_id": "compute_fscore",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_fscore(precision, recall):\n",
    "    \"\"\"\n",
    "    Returns lists of f-score values at different k values, where fscore[k] = the fscore@k\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    precision : np.ndarray\n",
    "        numpy array of precision values at different k values, where precision[k] = the\n",
    "        precision@k.\n",
    "    recall : np.ndarray\n",
    "        numpy array of recall values at different k values, where recall[k] = the\n",
    "        recall@k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Returns a numpy array of length equal to the length of the precision (and recall) parameter \n",
    "        array, where fscore[k] = the fscore@k.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b3df8f59d3ca23a3a4079b0132b5d34",
     "grade": true,
     "grade_id": "compute_fscore_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that precision_recall returns the correct output\"\"\"\n",
    "query, rel_movs = queries[1]\n",
    "ranked_movs = [m for m,_ in get_ranked_movies(query, movie_sims_cos)]\n",
    "precision, recall = precision_recall(ranked_movs, rel_movs)\n",
    "fscore = compute_fscore(precision, recall)\n",
    "\n",
    "assert fscore[0] == 0\n",
    "assert fscore.shape == (617,)\n",
    "assert recall.shape == (617,)\n",
    "assert sum(fscore) > 60 and sum(precision) < 100\n",
    "assert fscore[300] > 0.08 and precision[300] < 0.09\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad9471bdd48f4ce392f9e4a874e8f67f",
     "grade": false,
     "grade_id": "cell-5fa79dd748b5b0a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_fscore(matrix):\n",
    "    \"\"\"Plots the fscore curve given the similarity matrix\n",
    "    \n",
    "    Params: {matrix: np.ndarray}\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    query, rel_movs = queries[1]\n",
    "    ranked_movs = [m for m,_ in get_ranked_movies(query, movie_sims_cos)]\n",
    "\n",
    "    for query, rel_movs in queries:\n",
    "        ranked_movs = [m for m,_ in get_ranked_movies(query, matrix)] \n",
    "        precision, recall = precision_recall(ranked_movs, rel_movs)\n",
    "        fscore = compute_fscore(precision, recall)\n",
    "        plt.plot(range(1,len(fscore)+1), fscore)\n",
    "\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Fscore\")\n",
    "    plt.legend([q[0] for q in queries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "674df6e5ca3c45f65c6a295f649de65d",
     "grade": false,
     "grade_id": "cell-c4d2788c9a5990f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_fscore(movie_sims_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1108aaeae76be83aaf0c1f9f9f7f6abf",
     "grade": false,
     "grade_id": "cell-1787e98b89f4df33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_fscore(movie_sims_jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a95a0a16e592b89a334afc547b61b33",
     "grade": false,
     "grade_id": "cell-84d89d951f55f771",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5f (Free Response): Cosine Sim vs Jaccard Sim Fscore Plot Comparison\n",
    "\n",
    "Now that you've observed the fscore plots for both cosine similarity and jaccard similarity, please answer the following three questions:\n",
    "\n",
    "1. In what ways do the F-score curves provide more or different information about the retrievals compared to the precision-recall graphs?\n",
    "2. Are there any queries for which the F-score curves suggest different rankings of movies than the precision-recall graphs? If so, which queries and what are the differences?\n",
    "3. How would you interpret the shape of the F-score curves for a given query? What does a flat curve or a steeply increasing curve indicate about the rankings?\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da2717c0a6c8ef06235d9f425b6b4d89",
     "grade": true,
     "grade_id": "compute_fscore_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4324c3d7a35e61ff0a6239d6c691eadd",
     "grade": false,
     "grade_id": "cell-c28dc48be231ec6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "646bc21b152ff20d6574ffbf30748c24",
     "grade": false,
     "grade_id": "cell-109cf3608e5d5c15",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Evaluating our rankings with Mean Average Precision\n",
    "\n",
    "While precision/recall curves are a good tool to visualize how well our rankings are performing, in some contexts, it is ideal to come up with a single number that characterizes how well our ranking system is doing over all ground-truth queries. A commonly used statistic for ranking systems is _Mean Average Precision_. To compute this value, first, one must compute the average precision for each query and subsequent ranking output by the system. Next, one averages these values. We've filled in functions for both `average_precision` and `mean_average_precision` for you. At the end of this section, run the provided code, which will give you a single number that evaluates the performance of each ranking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea7b2d040521cef73e39c4e1487e75f7",
     "grade": false,
     "grade_id": "cell-d87b0cba955da30c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def average_precision(ranking_in, relevant):\n",
    "    '''\n",
    "    Arguments:\n",
    "        ranking_in: sorted ranking of movies, starting with the most most similar, and ending\n",
    "        with the least similar.\n",
    "        \n",
    "        relevant: iterable of movies relevant to the original query\n",
    "        \n",
    "    Returns:\n",
    "        average_precision: float corresponding to the AP statistic for this ranking and\n",
    "        this set of relevant docuemnts.\n",
    "    '''\n",
    "    rel_rank = sorted([ranking_in.index(r)+1 for r in relevant])\n",
    "    return np.mean([(i+1)*1./(r) for i, r in enumerate(rel_rank)])\n",
    "    \n",
    "    \n",
    "def mean_average_precision(queries_in, sims_mat):\n",
    "    '''\n",
    "    Arguments:\n",
    "        queries_in: a list of (query, [relevant documents]) pairs, representing the queries we\n",
    "        want to evaluate our ranking system against\n",
    "    \n",
    "        sims_mat: a movie by movie numpy array, where sims_mat[i,j] = sims_mat[j,i] = the similarity\n",
    "        of movies i and j\n",
    "    \n",
    "    Returns:\n",
    "        mean_average_precision: float corresponding to the average AP statistic for the input queries\n",
    "        and the similarity matrix\n",
    "    '''\n",
    "    rankings = [[movie_index_to_name[i] for i in np.argsort(sims_mat[movie_name_to_index[q],:])[::-1] if movie_index_to_name[i] != q]\n",
    "                for q,_ in queries]\n",
    "    aps = []\n",
    "    for i, results in enumerate(rankings):\n",
    "        ap = average_precision(results, queries_in[i][1])\n",
    "        aps.append(ap)\n",
    "        print(\"Query name:\", queries_in[i][0], \"//\", \"Avg. Precision: {:.3f}\".format(ap))\n",
    "    mean_avg_precision = 1.0 * sum(aps)/len(queries_in)\n",
    "    print(\"[Mean Average Precision: {:.3f}]\".format(mean_avg_precision))\n",
    "    return mean_avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83b13b209b3881c2c44da3cd2cede3b5",
     "grade": false,
     "grade_id": "cell-2edbcf40432cac11",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== cosine_sim with tfidf features ===\")\n",
    "mean_average_precision(queries, movie_sims_cos)\n",
    "print()\n",
    "print(\"=== jacc_sim ===\")\n",
    "mean_average_precision(queries, movie_sims_jac)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eb3125015d5204dce7d9b7ad9f743d3",
     "grade": false,
     "grade_id": "cell-d7af38aaebdd8447",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Rocchio Algorithm\n",
    "\n",
    "Great -- we have a baseline evaluation for how well our vector space tfidf information retrevial system does! Now, it is our job to do something better. Here, we will implement the Rocchio Algorithm and use it for pseudo relevance feedback.\n",
    "\n",
    "This is Rocchioâs query update rule for relevance feedback:\n",
    "\n",
    "$$\\overrightarrow{{q}_1} = a * \\overrightarrow{{q}_0} + b*\\frac{1}{|D_r|}\\sum_{d \\in D_r}{\\overrightarrow{d}}-c*\\frac{1}{|D_{nr}|}\\sum_{d \\in D_{nr}}{\\overrightarrow{d}}$$ \n",
    "\n",
    "In the above, $\\overrightarrow{{q}_0}$ is the initial query, $\\overrightarrow{{q}_1}$ is the updated query, $D_r$ is the set of relevant documents and $D_{nr}$ is the set of non-relevant documents. $a$, $b$, and $c$ are the update weights that correspond to the original query, the relevant queries, and the irrelevant queries, respectively. If after the update, there are negative values in $\\overrightarrow{{q}_1}$, they are set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6debc032cfa61f892ba98d7d467a987",
     "grade": false,
     "grade_id": "cell-31cb35d1cc54c927",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 6 (Code Completion): Rocchio\n",
    "Your job here is to implement the Rocchio Algorithm for relevance feedback. This function will test your understanding of how numpy arrays interact with variable types. For example, a + b where a is a python float and b is a numpy array will return a numpy array where each element of b has been shifted by a. While this particular function may or may not be needed to implement rocchio, it can be *very* helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2516265e51c6983dff8182d8b85d2abc",
     "grade": false,
     "grade_id": "rocchio",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rocchio(query, relevant, irrelevant, input_doc_matrix, \\\n",
    "            movie_name_to_index,a=.3, b=.3, c=.8, clip = True):\n",
    "    \"\"\"Returns a vector representing the modified query vector. \n",
    "    \n",
    "    Note: \n",
    "        If the `clip` parameter is set to True, the resulting vector should have \n",
    "        no negatve weights in it!\n",
    "        \n",
    "        Also, be sure to handle the cases where relevant and irrelevant are empty lists.\n",
    "        \n",
    "    Params: {query: String (the name of the movie being queried for),\n",
    "             relevant: List (the names of relevant movies for query),\n",
    "             irrelevant: List (the names of irrelevant movies for query),\n",
    "             input_doc_matrix: Numpy Array,\n",
    "             movie_name_to_index: Dict,\n",
    "             a,b,c: floats (weighting of the original query, relevant queries,\n",
    "                             and irrelevant queries, respectively),\n",
    "             clip: Boolean (whether or not to clip all returned negative values to 0)}\n",
    "    Returns: Numpy Array \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f54007ee3b71da9483009617238e25d8",
     "grade": false,
     "grade_id": "cell-02d9c6563f21a92a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Before we use the Rocchio algorithm for understanding pseduo-relevance feedback, we'll do a reality check and make sure that our implementation can improve performance on our existing queries. To augment our analysis, movie guru Ilan has also provided you with a list of irrelevant queries for each of our test queries. You'll notice that the irrelevant queries for each of the test queries are quite similar. This is because it's much more common for a pair of movies to be irrelevant, rather than relevant. In fact, it's not uncommon that *all* queries that are not relevant are *assumed to be irrelevant.*\n",
    "\n",
    "Here, though, we'll just use this small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df949ee25a37a54ea7add3ab4d6433e7",
     "grade": false,
     "grade_id": "cell-b606d1e4ff38c062",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "irrelevant  = [('the matrix', ['lone star',\n",
    "                               '2001: a space odyssey',\n",
    "                               'wall street',\n",
    "                               'the elephant man',\n",
    "                               'eternal sunshine of the spotless mind',\n",
    "                              'suburbia',\n",
    "                               'taking sides',\n",
    "                               'lake placid'\n",
    "                              ]),\n",
    "               ('star wars',  ['lone star',\n",
    "                               '2001: a space odyssey',\n",
    "                               'wall street',\n",
    "                               'the elephant man',\n",
    "                               'eternal sunshine of the spotless mind',\n",
    "                              'suburbia',\n",
    "                              'taking sides',\n",
    "                              'lake placid']),\n",
    "               ('a nightmare on elm street', ['lone star',\n",
    "                               '2001: a space odyssey',\n",
    "                               'wall street',\n",
    "                               'the elephant man',\n",
    "                               'eternal sunshine of the spotless mind',\n",
    "                                'suburbia',\n",
    "                                'taking sides',\n",
    "                                'lake placid']),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20e32678e4d9f33040c2888421547e00",
     "grade": true,
     "grade_id": "rocchio_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that rocchio returns the correct output\"\"\"\n",
    "query_vector = rocchio(\"the matrix\",[],irrelevant[0][1],doc_by_vocab,movie_name_to_index)\n",
    "query_vector2 = rocchio(\"star wars\",[],irrelevant[1][1],doc_by_vocab,movie_name_to_index)\n",
    "assert type(query_vector) == np.ndarray\n",
    "assert type(query_vector2) == np.ndarray\n",
    "assert sum(query_vector) > 3 and sum(query_vector) < 4\n",
    "assert sum(query_vector2) > 2 and sum(query_vector2) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2a6110b1d3c8b84d6ff7c352e6ad03e",
     "grade": false,
     "grade_id": "cell-48508f719504108e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7 (Code Completion): Leveraging Rocchio\n",
    "Your job is to leverage rocchio to return the top 10 highest ranked movies for each query, where the query vector is updated with rocchio. \n",
    "Return the dictionary with in the following format:\n",
    "```\n",
    "{'the matrix': [movie1,movie2,...,movie10],\n",
    " 'star wars': [movie1,movie2,...,movie10],\n",
    " 'a nightmare on elm street': [movie1,movie2,...,movie10]}\n",
    "```\n",
    "\n",
    "\n",
    "Note that the query itself should be excluded from the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0c1da39e25d4df4ebb853bdac853bab",
     "grade": false,
     "grade_id": "top_10_with_rocchio",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def top_10_with_rocchio(relevant_in, irrelevant_in, input_doc_matrix, \\\n",
    "            movie_name_to_index,movie_index_to_name,input_rocchio):\n",
    "    \"\"\"Returns a dictionary in the following format:\n",
    "    {\n",
    "        'the matrix': [movie1,movie2,...,movie10],\n",
    "        'star wars': [movie1,movie2,...,movie10],\n",
    "        'a nightmare on elm street': [movie1,movie2,...,movie10]\n",
    "    }\n",
    "    \n",
    "    Note: \n",
    "        You can assume that relevant_in[i][0] = irrelevant_in[i][0] \n",
    "        (i.e. the queries are in the same order). \n",
    "        \n",
    "        You should use the default rocchio parameters.\n",
    "        \n",
    "        You should NOT return the query itself in the list of most common\n",
    "        movies.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    relevant_in : (query: str, [relevant documents]: str list) list \n",
    "        List of tuples of the form:\n",
    "        tuple[0] = name of movie being queried (str), \n",
    "        tuple[1] = list of names of the relevant movies to the movie being queried (str list).\n",
    "    irrelevant_in : (query: str, [irrelevant documents]: str list) list \n",
    "        The same format as relevant_in except tuple[1] contains list of irrelevant movies instead.\n",
    "    input_doc_matrix : np.ndarray\n",
    "        The term document matrix of the movie transcripts. input_doc_mat[i][j] is the tfidf\n",
    "        of the movie i for the word j.\n",
    "    movie_name_to_index : dict\n",
    "         A dictionary linking the movie name (Key: str) to the movie index (Value: int). \n",
    "         Ex: {'movie_0': 0, 'movie_1': 1, .......}\n",
    "    movie_index_to_name : dict\n",
    "         A dictionary linking the movie index (Key: int) to the movie name (Value: str). \n",
    "         Ex: {0:'movie_0', 1:'movie_1', .......}\n",
    "    input_rocchio: function\n",
    "        A function implementing the rocchio algorithm. Refer to Q6 for the function \n",
    "        input parameters. Make sure you use the function's default parameters as \n",
    "        much as possible.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Returns the top ten highest ranked movies for each query in the format described above.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "114074f4c56c643d723dad186140b43b",
     "grade": false,
     "grade_id": "cell-e03266a4304b7776",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "movie_recommend_with_rocchio = top_10_with_rocchio(queries, irrelevant,\\\n",
    "                                                  doc_by_vocab,movie_name_to_index,\\\n",
    "                                                  movie_index_to_name,rocchio)\n",
    "for k,v in movie_recommend_with_rocchio.items():\n",
    "    print(k)\n",
    "    print(\"=\"*len(k))\n",
    "    [print(a) for a in v]\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e992d6b030d64c12e7b0215aeff77387",
     "grade": true,
     "grade_id": "top_10_with_rocchio_test",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that top_10_with_rocchio returns the correct output\"\"\"\n",
    "assert type(movie_recommend_with_rocchio) == dict\n",
    "assert \"the x files\" in movie_recommend_with_rocchio[\"the matrix\"]\n",
    "assert \"star wars: the empire strikes back\" in movie_recommend_with_rocchio[\"star wars\"]\n",
    "assert \"a nightmare on elm street 3: dream warriors\" in \\\n",
    "    movie_recommend_with_rocchio[\"a nightmare on elm street\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b31d57349b2a111a1652df3d259ddb7",
     "grade": false,
     "grade_id": "cell-6941eafd9a21a487",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7b (Code Completion): Leveraging Rocchio\n",
    "Your job is to implement `mean_average_precision_rocchio` below, to compute MAP using the Rocchio-updated query vectors.\n",
    "\n",
    "Your function should call `rocchio` and `average_precision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bb57b9631c3c437c6b45378d65de93b",
     "grade": false,
     "grade_id": "mean_average_precision_rocchio",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_average_precision_rocchio(relevant_in, irrelevant_in, input_doc_matrix, \\\n",
    "            movie_name_to_index, movie_index_to_name, input_rocchio):\n",
    "    \"\"\"Returns a float corresponding to the mean AP statistic for the Rocchio-updated input queries\n",
    "        and the similarity matrix\n",
    "    Note: \n",
    "        You can assume that relevant_in[i][0] = irrelevant_in[i][0] \n",
    "        (i.e. the queries are in the same order). \n",
    "        \n",
    "        You should use the default rocchio parameters.\n",
    "        \n",
    "        You should NOT include the query itself in the list of most common\n",
    "        movies.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    relevant_in : (query: str, [relevant documents]: str list) list \n",
    "        List of tuples of the form:\n",
    "        tuple[0] = name of movie being queried (str), \n",
    "        tuple[1] = list of names of the relevant movies to the movie being queried (str list).\n",
    "    irrelevant_in : (query: str, [irrelevant documents]: str list) list \n",
    "        The same format as relevant_in except tuple[1] contains list of irrelevant movies instead.\n",
    "    input_doc_matrix : np.ndarray\n",
    "        The term document matrix of the movie transcripts. input_doc_mat[i][j] is the tfidf\n",
    "        of the movie i for the word j.\n",
    "    movie_name_to_index : dict\n",
    "         A dictionary linking the movie name (Key: str) to the movie index (Value: int). \n",
    "         Ex: {'movie_0': 0, 'movie_1': 1, .......}\n",
    "    movie_index_to_name : dict\n",
    "         A dictionary linking the movie index (Key: int) to the movie name (Value: str). \n",
    "         Ex: {0:'movie_0', 1:'movie_1', .......}\n",
    "    input_rocchio: function\n",
    "        A function implementing the rocchio algorithm. Refer to Q6 for the function \n",
    "        input parameters. Make sure you use the function's default parameters as \n",
    "        much as possible.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Returns a float corresponding to the mean AP statistic for the Rocchio-updated input queries\n",
    "        and the similarity matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "838d5924b92c8bf7eeac6a76210878a7",
     "grade": true,
     "grade_id": "mean_average_precision_rocchio_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that mean_average_precision_rocchio returns the correct output\"\"\"\n",
    "mean_average_precision = mean_average_precision_rocchio(queries, irrelevant,\\\n",
    "                                                  doc_by_vocab,movie_name_to_index,\\\n",
    "                                                  movie_index_to_name,rocchio)\n",
    "assert mean_average_precision > 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a053640e658f1b5f79264d28c1cdfddd",
     "grade": false,
     "grade_id": "cell-798587bcc78c618e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The Rocchio addition to the system clearly adds a much larger level of precision as you are doing query\n",
    "modification that is based on relevant and irrelevant documents that are passed in. With the revision\n",
    "of this query this increases the search engine's recall, as well as the precision.\n",
    "\n",
    "**Note:**\n",
    "While our performance increased, it shouldn't be surprising that this is the case. At a high level, here is what we did: we took our original query vectors and moved them slightly closer to the \"ground truth\" relevant documents, and then used mean average precision to find that, indeed, the new query vector was closer to the true \"ground truth\" vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "037cf4f5b90018d3778228129c4638cd",
     "grade": false,
     "grade_id": "cell-6ecc56131c012635",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 8 (Free Response): Biggest Losses\n",
    "\n",
    "However, it is fun to see what words were given more or less weight for a given query according to the Rocchio modified query vector. Understand and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85f4da47e3be62a3e859d601b9754eba",
     "grade": false,
     "grade_id": "cell-95e722cbecf45266",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "q_o = doc_by_vocab[movie_name_to_index[queries[0][0]],:]\n",
    "q_m = rocchio(queries[0][0], queries[0][1], irrelevant[0][1],doc_by_vocab,movie_name_to_index)\n",
    "diffs = q_m-q_o\n",
    "args_diffs = np.argsort(diffs)\n",
    "losses = args_diffs[:10]\n",
    "print(\"Biggest losses:\")\n",
    "for l in losses:\n",
    "    print(\"{}:{:.3f}\".format(index_to_vocab[l], diffs[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "743ac3acdc1272714f6ff26fc7fc7001",
     "grade": false,
     "grade_id": "cell-4c62999080fa04a9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the cell below, explain what the code is doing and what the findings show\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\">Write your answer in the provided cell below</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4121d5871bec3ace4affa352fbe3e56a",
     "grade": true,
     "grade_id": "biggest_loss_ans",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95736a0938d1b2e30c339c221560f6d8",
     "grade": false,
     "grade_id": "cell-78671e18533b1ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div style=\"border-bottom: 4px solid #AAA; padding-bottom: 6px; font-size: 16px; font-weight: bold;\"></div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "LangInfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "98ca369906f7b70fe64afc73e530e378539d08b0e41e494e07f1eb5e7a313c26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
