{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7c4ba72adf31577",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 1: Keeping Up With Social Information (Part 1)\n",
    "## Â© Cristian Danescu-Niculescu-Mizil 2023\n",
    "## CS/INFO 4300 Language and Information\n",
    "## Due by midnight on Wednesday February 1st\n",
    "\n",
    "This assignment is **individual**.\n",
    "\n",
    "In this assignment we are practicing with post-processing on a conversational dataset taken from the reality TV show \"Keeping Up With The Kardashians\" and gathering some basic statistics about it. \n",
    "\n",
    "In the next assignment (Assignment 2) we will extend these tools to analyze conversational behavior.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "This project aims to help you get comfortable working with the following tools / technologies / concepts:\n",
    "\n",
    "* word tokenization\n",
    "* histogram plotting using `matplotlib`\n",
    "* character analysis via conversational language\n",
    "* familiarize yourself with basic numpy usage\n",
    "\n",
    "**Academic Integrity and Collaboration**\n",
    "\n",
    "Note that these projects should be completed individually. As a result, all University-standard academic integrity guidelines must be followed.\n",
    "\n",
    "**Guidelines**\n",
    "\n",
    "All cells that contain the blocks that read `# YOUR CODE HERE` are editable and are to be completed to ensure you pass the test-cases. Make sure to write your code where indicated.\n",
    "\n",
    "All cells that read `YOUR ANSWER HERE` are free-response cells that are editable and are to be completed.\n",
    "\n",
    "You may use any number of notebook cells to explore the data and test out your functions, although you will only be graded on the solution itself.\n",
    "\n",
    "\n",
    "You are unable to modify the read-only cells.\n",
    "\n",
    "You should also use Markdown cells to explain your code and discuss your results when necessary.\n",
    "Instructions can be found [here](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "All floating point values should be printed with **2 decimal places** precision. You can do so using the built-in round function.\n",
    "\n",
    "**Grading**\n",
    "\n",
    "For code-completion questions you will be graded on passing the public test cases we have included, as well as any hidden test cases that we have supplemented within a given amount of time to ensure that your logic is correct.\n",
    "\n",
    "Your solution to A1 should finish running in **less than 5 minutes**.\n",
    "For your information, it takes less than 1 minute to run our solution for the entire A1.\n",
    "Also make sure to remove any **redundant print statements** to speed things up and prevent generating unnecessary outputs.\n",
    "\n",
    "For free-response questions you will be manually graded on the quality of your answer.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "You are expected to submit this .ipynb as your submission for Assignment 1. \n",
    "\n",
    "In addition please submit an html copy of the notebook (You can create this by clicking File > Download as > HTML (.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d381da04cb83dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### A0 Review\n",
    "\n",
    "Recall the learning objectives of A0:\n",
    "\n",
    "- The Jupyter Notebook environment\n",
    "- Recap of Python syntax and basic data structures\n",
    "- virtualenv environment for package dependencies\n",
    "\n",
    "We used the BeautifulSoup library to extract episode titles, timestamps, and character speech from HTML files, then stored this data in Python Dictionaries. Finally, we conducted preliminary checks of our data to test its usability (i.e. checking for duplicate transcripts and converting nickname \"Rob\" to \"Robert\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97b1c5e4f7df612c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import bs4\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7114417ecfb57e62",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure that your kernel is using Python3\n",
    "assert sys.version_info.major == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ced8dfe7b8cc493",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Preliminary Data Cleansing\n",
    "**Note: The following content is for you to review to understand how we cleaned and prepared the data for the analysis below.**\n",
    "\n",
    "We will be continuing where we left off from Assignment 0. \n",
    "\n",
    "### Removing duplicates\n",
    "If you are to examine the original transcripts you will see that many of them are near-duplicates, but most are not *perfect* duplicates. This is problematic, because we cannot simply remove identical transcripts.  Furthermore, we cannot just throw away documents that have large overlap, because we would be throwing away the valuable data that is *not* overlapping.\n",
    "\n",
    "We therefore have to treat the transcripts as sequences, rather than as entire documents, and just remove subsequences that overlap.\n",
    "\n",
    "We therefore used a standard python `difflib` package to write the `find_overlaps` function:\n",
    "\n",
    "```python\n",
    "import difflib\n",
    "def find_overlaps(transcript_a, transcript_b, threshold=5):\n",
    "    \"\"\"Find and return the indices of overlapping subsequences between the two transcripts.\n",
    "    Only return overlapping sequences that consist of at least `threshold` entries.\"\"\"\n",
    "    \n",
    "    # We consider that two transcripts overlap when the messages\n",
    "    # and the speakers are the same, but not the timestamp.\n",
    "    \n",
    "    # Massage the transcripts to disregard timestamp information.\n",
    "    # note that a tuple is hashable, so is okay to use for difflib's SequenceMatcher class.\n",
    "    msgs_a = [(m['speaker'], m['text']) if m is not None else None\n",
    "              for m in transcript_a]\n",
    "    msgs_b = [(m['speaker'], m['text']) if m is not None else None\n",
    "              for m in transcript_b]\n",
    "    matcher = difflib.SequenceMatcher(None, msgs_a, msgs_b)\n",
    "    return list(filter(lambda tup: tup[2] >= threshold, matcher.get_matching_blocks()))\n",
    "```\n",
    "\n",
    "We now use the function above to remove duplicate subsequences. At each step, assume we have a list of \"good\" transcripts that have already been processed. When considering a new transcript, we first remove all subsequences that overlap with any of the already processed ones. Then, we split up the chunks that are not removed, and consider each of them a new transcript.\n",
    "\n",
    "```python\n",
    "deduped_transcripts = []\n",
    "all_keys = sorted(transcripts.keys())\n",
    "\n",
    "for key in all_keys:\n",
    "    transcript = transcripts[key]\n",
    "    for _, good_transcript in deduped_transcripts:\n",
    "        overlaps = find_overlaps(transcript, good_transcript)\n",
    "        for idx_a, _, size in overlaps:\n",
    "            transcript[idx_a:idx_a + size] = [None] * size\n",
    "    \n",
    "    for is_not_none, group in groupby(transcript, lambda x: x is not None):\n",
    "        if is_not_none:\n",
    "            subtranscript = list(group)\n",
    "            deduped_transcripts.append((key, subtranscript))\n",
    "```\n",
    "\n",
    "The `deduped_transcripts` are what you are now analyzing for the rest of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e31168ecff8580a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## We are loading the pickle file that contains all the deduped transcripts from Assignment 0\n",
    "with open('deduped_transcripts.pickle','rb') as f:\n",
    "    deduped_transcripts = pickle.load(f)\n",
    "## We are also loading a pickle file of the titles file that we determined in the beginning of Assignment 0\n",
    "with open('titles.pickle','rb') as f:\n",
    "    titles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fcb040d322bffc2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Language analysis\n",
    "\n",
    "## Identifying the words\n",
    "It's time to get down to the bread-and-butter of language analysis: the words used.  For simplification, **we consider a word to be a sequence of alphabetical characters. Treat all other characters as delimiters and do not return them.**\n",
    "\n",
    "\n",
    "## Question 1 (Code Completion): Tokenization \n",
    "\n",
    "In the cell below: *Write a function to 'tokenize' a string into the constituent words*. \n",
    "\n",
    "You **must** use regex to satisfy the function specification. We recommend you leverage `re.findall`. \n",
    "\n",
    "Hint: Check out this online regex calculator: [here](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "tokenize",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything.\n",
    "    Requirement: Use Regex to satisfy this function\n",
    "    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    # find the expression that matches a word using regex\n",
    "    tokens = re.findall(r'[a-z]+', text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "tokenize_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that tokenize returns the correct output\"\"\"\n",
    "assert tokenize(\"It's time 2 get down to the bread-and-butter\") == \\\n",
    "    ['it', 's', 'time', 'get', 'down', 'to', 'the', 'bread', 'and', 'butter']\n",
    "assert tokenize(\"Life, Liberty, & the Pursuit of Happiness\") == \\\n",
    "    ['life', 'liberty', 'the', 'pursuit', 'of', 'happiness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d753d9a79894c197",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2 (Code Completion): Tokenization of Entire Transcript\n",
    "\n",
    "In the cell below write a function that *tokenizes all of the text in an input transcript (given a method for tokenizing strings)*, producing a list of all tokens found in the entire transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "tokenize_transcript",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_transcript(tokenize_method,input_transcript):\n",
    "    \"\"\"Returns a list of tokens contained in an entire transcript.\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             input_transcript: Tuple}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    tokens = [] \n",
    "    # input_transcript[1] is a list of dictionaries\n",
    "    # each dictionary has a key 'text' and a value that is a string\n",
    "    for transcript in input_transcript[1]:\n",
    "        text = transcript['text']\n",
    "        tokens += tokenize_method(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c3d95463c66bd217",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deduped_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "tokenize_transcript_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that tokenize returns the correct output\"\"\"\n",
    "assert len(tokenize_transcript(tokenize,deduped_transcripts[0])) > 6000 and \\\n",
    "    len(tokenize_transcript(tokenize,deduped_transcripts[0])) < 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-659f2ecbe807eaef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3 (Code Completion) Number of Tokens\n",
    "In the cell below write a function to *count how many tokens are used in the deduplicated transcripts in total*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "num_dedup_tokens",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def num_dedup_tokens(tokenize_method,tokenize_transcript_method,input_transcripts):\n",
    "    \"\"\"Returns number of tokens used in an entire transcript\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "             tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "             input_transcripts: Tuple List}\n",
    "    Returns: Integer\n",
    "    \"\"\"\n",
    "    token_num = 0\n",
    "    for transcript in deduped_transcripts:\n",
    "        token_num += len(tokenize_transcript_method(tokenize_method,transcript))\n",
    "    return token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "num_dedup_tokens_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that num_dedup_tokens returns the correct output\"\"\"\n",
    "assert num_dedup_tokens(tokenize,tokenize_transcript,deduped_transcripts) > 200000 and \\\n",
    "    num_dedup_tokens(tokenize,tokenize_transcript,deduped_transcripts) < 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed6fa2191c79a632",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4 (Code Completion) Number of Distinct Words\n",
    "\n",
    "In the cell below write a function to *count how many distinct words are in the deduplicated transcripts in total*. \n",
    "\n",
    "Hint: Use a *set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "num_distinct_words",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def num_distinct_words(tokenize_method,tokenize_transcript_method,input_transcripts):\n",
    "    \"\"\"Returns number of distinct tokens used in an entire transcript\n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "             tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "             input_transcripts: Tuple List}\n",
    "    Returns: Integer\n",
    "    \"\"\"\n",
    "    distinct_token = set()\n",
    "    for transcript in deduped_transcripts:\n",
    "        distinct_token.update(tokenize_transcript_method(tokenize_method,transcript))\n",
    "    return len(distinct_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "num_distict_words_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that num_dedup_tokens returns the correct output\"\"\"\n",
    "assert num_distinct_words(tokenize,tokenize_transcript,deduped_transcripts) > 8000 and \\\n",
    "    num_distinct_words(tokenize,tokenize_transcript,deduped_transcripts) < 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d5da2bb92583201",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Question 5 (Code Completion) Word Episode Counts\n",
    "\n",
    "This question is asking you to build a dictionary `word_episode_count[word]` = *number of episodes in which the word appears*. \n",
    "\n",
    "*Note: Keep in mind that the de-duplicated transcripts don't have unique titles!\n",
    "Recall from A0 that **one episode corresponds to exactly one title** (but not necessarily to only one transcript).*\n",
    "\n",
    "**Your code should ideally take less than 1 second (or a few seconds) to run. If it does not, then you should be able to find a better answer.**\n",
    "\n",
    "In the cell below write a function that counts: *for each distinct (unique) word, in how many different episodes does it appear?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "build_word_episode_count",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_word_episode_count(tokenize_method,tokenize_transcript_method,input_transcripts,input_titles):\n",
    "    \"\"\"Returns a dictionary with the number of episodes each distinct word appears\n",
    "        Params: {tokenize_method: Function (a -> b),\n",
    "                 // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "                 tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "                 input_transcripts: Tuple List,\n",
    "                 input_titles: Dictionary}\n",
    "        Returns: Dict\n",
    "    \"\"\"\n",
    "    # a dictionary with the number of episodes each distinct word appears, initialized to 0\n",
    "    distinct_tokens = set()\n",
    "    for transcript in input_transcripts:\n",
    "        distinct_tokens.update(tokenize_transcript_method(tokenize_method,transcript))\n",
    "\n",
    "    word_episode_count = dict.fromkeys(distinct_tokens,0)\n",
    "\n",
    "    # print(word_episode_count)\n",
    "\n",
    "    # a set of distinct titles\n",
    "    distinct_titles = {value for value in titles.values()}\n",
    "\n",
    "    for title in distinct_titles:\n",
    "\n",
    "        # a set of distinct tokens in an episode\n",
    "        tokens_in_episode=set()\n",
    "\n",
    "        # find the first transcripts with the title\n",
    "        for index in range(len(input_transcripts)):\n",
    "            code = input_transcripts[index][0]\n",
    "            if (titles[code] == title):\n",
    "                # tokenize the transcript\n",
    "                tokens = tokenize_transcript_method(tokenize_method,input_transcripts[index])\n",
    "                tokens_in_episode.update(tokens)\n",
    "        \n",
    "\n",
    "        # update the word_episode_count\n",
    "        for token in tokens_in_episode:\n",
    "            word_episode_count[token] += 1\n",
    "    return word_episode_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a14a82efd4f0e23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_episode_count = build_word_episode_count(tokenize,tokenize_transcript,deduped_transcripts,titles)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "build_word_episode_count_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_word_episode_count returns the correct output\"\"\"\n",
    "assert word_episode_count['quarter'] == 2\n",
    "assert word_episode_count['made'] == 40\n",
    "assert word_episode_count['never'] == 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (Code Completion) Distribution Analysis\n",
    "\n",
    "For this question, you will be summarizing the word episode counts you found above by combining them into a *distribution* of the number of episodes in which words appear. That is: *how many words appear in only one episode, how many appear in two episodes, and so on?*\n",
    "\n",
    "Specifically, you will build a dictionary `word_episode_distribution[n]` = *number of words that appear in exactly* `n` *episodes*.\n",
    "\n",
    "In the cell below fulfill the specifications above and have the function: *produce a distribution of word episode counts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_episode_distribution(input_word_counts):\n",
    "    \"\"\"Returns a dictionary that counts how many words appear in exactly a given number of episodes\n",
    "        Params: {input_word_counts: Dict}\n",
    "        Returns: Dict\n",
    "    \"\"\"\n",
    "\n",
    "    # a dictionary that counts how many words appear in exactly a given number of episodes\n",
    "    word_episode_distribution = dict.fromkeys(set(input_word_counts.values()),0)\n",
    "\n",
    "    for count in input_word_counts.values():\n",
    "        word_episode_distribution[count] += 1\n",
    "    return word_episode_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_episode_distribution = build_word_episode_distribution(word_episode_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that build_word_episode_distribution returns the correct output\"\"\"\n",
    "assert word_episode_distribution[1] > 4000 and word_episode_distribution[1] < 4500\n",
    "assert word_episode_distribution[3] > 700 and word_episode_distribution[3] < 800\n",
    "assert word_episode_distribution[15] > 30 and word_episode_distribution[15] < 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88307b2867a85c7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 6b (Free Response): Visualizing the distribution\n",
    "\n",
    "When working with distributions (like we're doing here), it can be helpful to *visualize* the distribution in order to get a better intuition. This can be achieved through the use of *histograms*, so in this question you will create one. \n",
    "\n",
    "Each bin of the histogram  (the *x axis*) should correspond to the *number of episodes* in which a word is mentioned, and the *y axis* should show the *number of words* in each bin. \n",
    "\n",
    "Note: Use the default matplotlib settings. You may find this tutorial helpful: https://matplotlib.org/3.0.2/tutorials/introductory/pyplot.html\n",
    "\n",
    "Create a new cell with the histogram below. Then, give an analysis of the distribution: What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "build_word_episode_count_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(word_episode_distribution.keys()), list(word_episode_distribution.values()))\n",
    "plt.xlabel('number of episodes')\n",
    "plt.ylabel('number of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b78c4d64909ac049",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7 (Code Completion): Good Types\n",
    "\n",
    "As we have now seen, there are many *infrequent* words that occur only in one episode, and never again afterwards. When analyzing language use---for instance, if we wanted to make sense of how various characters tend to behave---we might not put too much stock in such infrequent words (maybe Kim was just having a bad day). This question deals with the step of filtering out infrequent words.\n",
    "\n",
    "In the cell below: build an alphabetically sorted list of all words that appear in more than one episode. We'll refer to these words as *good types*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "output_good_types",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def output_good_types(input_word_counts):\n",
    "    \"\"\"Returns a list of good types in alphabetically sorted order\n",
    "        Params: {input_word_counts: Dict}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    good_types = []\n",
    "    for word in input_word_counts:\n",
    "        if input_word_counts[word] > 1:\n",
    "            good_types.append(word)\n",
    "    return sorted(good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f0bb8ce404b6b24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "good_types = output_good_types(word_episode_count)\n",
    "n_good_types = len(good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "output_good_types_test",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that output_good_types returns the correct output\"\"\"\n",
    "assert n_good_types > 4500 and n_good_types < 5000\n",
    "assert good_types[0:5] == ['a','aah','ability','able','about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-800cf3466c807a90",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 7b (Free Response): Good Types\n",
    "\n",
    "In the cell below answer the following: *How many good_types there are? What are the first 10 in alphabetical order?*\n",
    "\n",
    "Please write your answer in code and use Python's `print()` function, **NOT** markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "output_good_types_ans",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "aah\n",
      "ability\n",
      "able\n",
      "about\n",
      "absolute\n",
      "absolutely\n",
      "absurd\n",
      "accent\n",
      "accept\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,10):\n",
    "    print(good_types[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd25b6c0ee0f7aa7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "From now on, the use of array data structures from `numpy` is required in some places. If you've never used numpy before, it will take some time to get used to. In general, numpy is a library that you can use to handle vector/matrix/tensor operations, including creation, modification, and compositions (add, sub, mul, etc.). For example, try running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a35c29d2a336c43f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 6 2]]\n",
      "[[1 2]\n",
      " [5 3]\n",
      " [1 1]]\n",
      "3\n",
      "[[14 11]\n",
      " [36 28]]\n",
      "[5 8 5]\n",
      "[3 8 2]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [4,6,2]]) \n",
    "B = np.array([[1,2],[5,3],[1,1]])\n",
    "print(A) #A is a 2x3 matrix\n",
    "print(B) #B is a 3x2 matrix\n",
    "print(A[0,2]) # Prints the value contained in the first row, third column of matrix A\n",
    "print(A.dot(B)) #A.dot(B) multiplies the two matrices, producing a 2 by 2 matrix\n",
    "print(np.sum(A, axis=0)) #np.sum(A, axis=0) sums along columns\n",
    "print(np.sum(B, axis=1)) #np.sum(B, axis=1) sums along rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be083b1847639a79",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You're welcome to find your own resources to learn more about numpy (there are lots of them) but one good introduction is [Justin Johnson's writeup](http://cs231n.github.io/python-numpy-tutorial/#numpy).\n",
    "\n",
    "We will be using vectors and arrays with *n_good_types* columns, such that each good type corresponds to a column, in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f473f9073805a3f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 8 (Code Completion): Word Frequencies\n",
    "\n",
    "Previously, we characterized words according to how many episodes they appeared in. For this question, we will take a different approach: computing the rate (or *frequency*) at which a word appears across the entire corpus (note: a corpus is a collection of written texts, in this case the transcripts of a reality TV show).\n",
    "\n",
    "In the cell below, complete the function to *find the word frequency of all \"good types\"* in descending order.\n",
    "\n",
    "For this question only, **round your solution to 5 decimals**.\n",
    "\n",
    "What can we say about the most frequently used words? Briefly consider, would you expect to find the same ordering of frequent words in, say, the NY Times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "create_ranked_good_types",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_ranked_good_types(tokenize_method,tokenize_transcript_method,input_transcripts,input_good_types):\n",
    "    \"\"\"Returns a list of good types in reverse sorted order in the form:\n",
    "        [(word_1,word_frequency_1),\n",
    "        ...\n",
    "        (word_10,word_frequency_10)]\n",
    "        Params: {tokenize_method: Function (a -> b),\n",
    "                 // Note: Below type means a function that takes two arguments, the first of which is a function.\n",
    "                 tokenize_transcript_method: Function ((Function(a -> b), c) -> d),\n",
    "                 input_transcripts: Tuple List,\n",
    "                 input_good_types: List}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    word_frequency = dict.fromkeys(input_good_types,0)\n",
    "\n",
    "    # total number of corpus\n",
    "    total_num = 0\n",
    "\n",
    "    for transcript in input_transcripts:\n",
    "        tokens = tokenize_transcript_method(tokenize_method,transcript)\n",
    "        for token in tokens:\n",
    "            total_num += 1\n",
    "            if token in word_frequency:\n",
    "                word_frequency[token] += 1\n",
    "    \n",
    "\n",
    "\n",
    "    # calculate the frequency of each word\n",
    "    for word in word_frequency:\n",
    "        word_frequency[word] = round(word_frequency[word]/total_num,5)\n",
    "\n",
    "    # sort the word_frequency in reverse order\n",
    "    ranked_good_types = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "    return ranked_good_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bb7a64aaa1b1620",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ranked_frequencies = create_ranked_good_types(tokenize,tokenize_transcript,deduped_transcripts,good_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-098c443e5f1380a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Suggestion: Sum the word frequencies found for all good types. Consider why they do not add to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "create_ranked_good_types_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_ranked_good_types returns the correct output\"\"\"\n",
    "words_only = [word for word, freq in ranked_frequencies]\n",
    "frequencies_only = np.array([freq for word, freq in ranked_frequencies])\n",
    "assert words_only[0:2] == ['i','you']\n",
    "assert np.isclose(frequencies_only[0:2], \n",
    "            np.array([0.04858, 0.03797]), atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the autograder uses some additional numpy utility functions. You don't need to be familiar with these right now, but in case you are curious:\n",
    "\n",
    "- `np.isclose` is a useful function for comparing floats, that gets around the problem of floating-point imprecision;\n",
    "- `.all()` is True whenever all the entries of a boolean array are True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f52d6dd8641a84e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Characterizing characters' language\n",
    "\n",
    "Moving on, we will only be considering a subset of characters, arguably the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87f62951498c6a21",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "good_speakers = [u'BRUCE',\n",
    "                 u'JONATHAN',\n",
    "                 u'KHLOE',\n",
    "                 u'KIM',\n",
    "                 u'KOURTNEY',\n",
    "                 u'KRIS',\n",
    "                 u'ROBERT',\n",
    "                 u'SCOTT']\n",
    "\n",
    "n_speakers = len(good_speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e73cf04c0942e6e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9 (Code Completion): Character Word Occurrences\n",
    "\n",
    "In the cell below you will be asked to determine *how often each word is said by each character*.\n",
    "\n",
    "This function **requires** you to return a numpy array of shape `n_speakers` rows by `n_good_types` columns, such that the entry `(i,j)` indicates how many times speaker i says word j. **You will lose points if you do not use numpy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "create_word_freq_array",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_word_occurrence_matrix(\n",
    "    tokenize_method,\n",
    "    input_transcripts,\n",
    "    input_speakers,\n",
    "    input_good_types):\n",
    "    \"\"\"Returns a numpy array of shape n_speakers by n_good_types such that the \n",
    "    entry (ij) indicates how often speaker i says word j.\n",
    "    \n",
    "    Params: {tokenize_method: Function (a -> b),\n",
    "             input_transcripts: Tuple List,\n",
    "             input_speakers: List,\n",
    "             input_good_types: List}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    # initialize the matrix\n",
    "    word_occurrence_matrix = np.zeros((n_speakers,n_good_types))\n",
    "\n",
    "    for transcript in input_transcripts:\n",
    "        # deal with each sentence\n",
    "        for sentence in transcript[1]:\n",
    "            # get the speaker name\n",
    "            speaker = sentence['speaker']\n",
    "            if speaker in input_speakers:\n",
    "                # tokenize the sentence\n",
    "                tokens = tokenize_method(sentence['text'])\n",
    "                for token in tokens:\n",
    "                    if token in input_good_types:\n",
    "                        word_occurrence_matrix[input_speakers.index(speaker)][input_good_types.index(token)] += 1\n",
    "        \n",
    "    return word_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4229598e63922f3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_matrix = create_word_occurrence_matrix(tokenize,deduped_transcripts,good_speakers,good_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "create_word_freq_array_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_word_freq_array returns the correct output\"\"\"\n",
    "assert sum(word_matrix[0]) > 26000.0\n",
    "assert sum(word_matrix[:,3]) > 40 and sum(word_matrix[:,3]) < 50\n",
    "assert type(word_matrix) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55cdd926451a2321",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9b (Free Response): Character Word Occurrences\n",
    "\n",
    "In the cell below, output the *top 10 most occurring words used by each character*, again in descending order, in the following format (if you encounter ties, then any ordering is acceptable):\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME_A\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "\n",
    "CHARACTER_NAME_B\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "create_word_freq_array_ans",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE\n",
      "i\n",
      "you\n",
      "the\n",
      "s\n",
      "to\n",
      "it\n",
      "a\n",
      "and\n",
      "that\n",
      "this\n",
      "JONATHAN\n",
      "i\n",
      "you\n",
      "to\n",
      "the\n",
      "s\n",
      "a\n",
      "it\n",
      "and\n",
      "like\n",
      "this\n",
      "KHLOE\n",
      "i\n",
      "you\n",
      "to\n",
      "and\n",
      "s\n",
      "a\n",
      "the\n",
      "it\n",
      "that\n",
      "like\n",
      "KIM\n",
      "i\n",
      "you\n",
      "to\n",
      "and\n",
      "s\n",
      "the\n",
      "it\n",
      "a\n",
      "that\n",
      "like\n",
      "KOURTNEY\n",
      "i\n",
      "to\n",
      "you\n",
      "and\n",
      "s\n",
      "it\n",
      "the\n",
      "a\n",
      "that\n",
      "t\n",
      "KRIS\n",
      "i\n",
      "you\n",
      "to\n",
      "s\n",
      "the\n",
      "a\n",
      "and\n",
      "it\n",
      "that\n",
      "t\n",
      "ROBERT\n",
      "i\n",
      "to\n",
      "you\n",
      "s\n",
      "and\n",
      "a\n",
      "the\n",
      "just\n",
      "it\n",
      "that\n",
      "SCOTT\n",
      "i\n",
      "you\n",
      "to\n",
      "s\n",
      "the\n",
      "a\n",
      "it\n",
      "that\n",
      "and\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "for speaker in range(n_speakers):\n",
    "    print(good_speakers[speaker])\n",
    "    word_array = np.array(word_matrix[speaker])\n",
    "    for i in range(10):\n",
    "        index = np.argmax(word_array)\n",
    "        print(good_types[index])\n",
    "        word_array[index] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe5bd652435d4c18",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 9c (Code Completion): Character Word Occurrences\n",
    "\n",
    "Using your `word_matrix`, identify the characters (in descending order of number of occurrences) who have said **all** of the following words: `[\"brother\", \"book\", \"best\"]`. \n",
    "\n",
    "Please note: a character must have spoken each word at least once to be included in your answer (if you encounter ties, then any ordering is acceptable).\n",
    "\n",
    "In the cell below, output your answer in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "brother\n",
    "CHARACTER_NAME_A occurrences_brother_A\n",
    "CHARACTER_NAME_B occurrences_brother_B\n",
    "...\n",
    "\n",
    "book\n",
    "CHARACTER_NAME_A occurrences_book_A\n",
    "CHARACTER_NAME_B occurrences_book_B\n",
    "...\n",
    "\n",
    "best\n",
    "CHARACTER_NAME_A occurrences_best_A\n",
    "CHARACTER_NAME_B occurrences_best_B\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1c70f3e0648642d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "interesting_words = [\"brother\", \"book\", \"best\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51c7d3273a979ae8",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brother\n",
      "BRUCE 1\n",
      "KIM 44\n",
      "KOURTNEY 3\n",
      "KRIS 1\n",
      "SCOTT 2\n",
      "\n",
      "book\n",
      "BRUCE 2\n",
      "KIM 4\n",
      "KOURTNEY 1\n",
      "KRIS 6\n",
      "SCOTT 2\n",
      "\n",
      "best\n",
      "BRUCE 16\n",
      "KIM 20\n",
      "KOURTNEY 20\n",
      "KRIS 27\n",
      "SCOTT 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interesting_words_index = [good_types.index(word) for word in interesting_words]\n",
    "word_array = word_matrix[...,interesting_words_index]\n",
    "\n",
    "# print(word_array)\n",
    "\n",
    "sayitall_speaker = []\n",
    "\n",
    "for speaker in range(n_speakers):\n",
    "    if(word_array[speaker][0] > 0 and  word_array[speaker][1] > 0 and word_array[speaker][1] > 0 ):\n",
    "        sayitall_speaker.append(good_speakers[speaker])\n",
    "\n",
    "for word in interesting_words:\n",
    "    print(word)\n",
    "    for speaker in sayitall_speaker:\n",
    "        print(speaker, int(word_array[good_speakers.index(speaker)][interesting_words.index(word)]))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a3402cec2421178",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 10 (Code Completion): Word Character Counts\n",
    "\n",
    "In the questions below you will now be asked to determine *how many characters have used a specific word*? For example, how many characters have ever said \"botox\" in the show?\n",
    "\n",
    "The function below will require you to use the `word_matrix` from above to return a 1-D numpy array that reports the number of `good_speakers` that have uttered each word within `good_types`.\n",
    "The i-th entry of your answer array `word_character_count_array[i]` should be the number of characters that have uttered the word `good_types[i]`.\n",
    "\n",
    "Hint! Numpy is your friend :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "create_word_character_count_array",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_word_character_count_array(word_freq_matrix):\n",
    "    \"\"\"Returns a numpy array of shape (n_good_types,) such that the \n",
    "    entry i indicates how many good_speakers have uttered word i.\n",
    "    \n",
    "    Params: { word_freq_matrix: a numpy matrix of shape (n_speakers, n_good_types) }\n",
    "    \n",
    "    Hint: You may want to consult the numpy documentation to make this easy.\n",
    "    \"\"\"\n",
    "    word_freq_matrix = word_freq_matrix.astype(bool).astype(int)\n",
    "    word_character_count_array = np.sum(word_freq_matrix, axis=0)\n",
    "    return word_character_count_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11c569836da724f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "word_character_count_array = create_word_character_count_array(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "create_word_character_count_array_tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_character_word_count_array returns the correct output\"\"\"\n",
    "assert type(word_character_count_array) == np.ndarray\n",
    "assert word_character_count_array[0] == 8\n",
    "assert word_character_count_array[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb8e24a9dd852d39",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 10b (Free Response): Word Character Counts\n",
    "\n",
    "What are the first 10 words (sorted in alphabetical order) that are said by **at least 5** characters?\n",
    "\n",
    "In the cell below, give your answer in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "word_1\n",
    "word_2\n",
    "...\n",
    "word_10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f64719c7d57ec17e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "able\n",
      "about\n",
      "absolutely\n",
      "accept\n",
      "access\n",
      "account\n",
      "act\n",
      "acting\n",
      "active\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in range(10):\n",
    "    while not(word_character_count_array[index] >= 5):\n",
    "        index += 1\n",
    "    print(good_types[index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e055c738cea3d3b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 11 (Code Completion): Specific Word Usage by Character\n",
    "\n",
    "The exercise in 9b didn't help much in understanding each character's diction, because common words are used too commonly anyway. We want to give more weight to less frequent (that is, more \"specific\") words, as they carry more information on the particularities of the characters.\n",
    "\n",
    "A simple way to do this is to score the words according to the ratio between how often a given character said the word and how often any of the *good speakers* said it.  \n",
    "\n",
    "This can be accomplished by dividing each column in the `word_array` matrix by its sum.\n",
    "\n",
    "**Note: as some words might never be said by the key characters we are considering, add 1 to the sum of each column to avoid division by 0**\n",
    "\n",
    "In the cell below, write a function that uses the methodology described above to return a weighted numpy array of *specific* words used by each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "create_weighted_word_freq_array",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_weighted_word_freq_array(input_word_array):\n",
    "    \"\"\"Returns a numpy array of shape n_speakers by n_good_types such that the \n",
    "    entry (ij) indicates how often speaker i says word j weighted by the above ratio.\n",
    "    \n",
    "    Note: You must add 1 to the sum of each column to avoid divison by 0 issues.\n",
    "    \n",
    "    Params: {input_word_array: Numpy Array}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    word_count_array = np.sum(input_word_array, axis=0) + 1\n",
    "    weighted_word_freq_array = input_word_array / word_count_array\n",
    "    return weighted_word_freq_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f3e0cd3d8309ecea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "weighted_words = create_weighted_word_freq_array(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "create_weighted_word_freq_array_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograder test. Here we can test the function you just wrote above.\n",
    "\"\"\"Check that create_word_freq_array returns the correct output\"\"\"\n",
    "assert sum(weighted_words[:,7]) > 0.7\n",
    "assert type(weighted_words) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b2b74adae027c815",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 11b (Free Response): Specific Word Usage by Character\n",
    "\n",
    "Use the next cell to output the top 10 most *specific* words (in descending order) used by each character in the following format:\n",
    "\n",
    "**Answer format**:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME_A\n",
    "score_1 word_1\n",
    "score_2 word_2\n",
    "...\n",
    "score_10 word_10\n",
    "\n",
    "CHARACTER_NAME_B\n",
    "score_1 word_1\n",
    "score_2 word_2\n",
    "...\n",
    "score_10 word_10\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top(input_words, show_scores):\n",
    "    \"\"\"Prints the top 10 words in the input_words array.\n",
    "    \n",
    "    Params: {input_words: Numpy Array,\n",
    "             show_scores: Boolean}\n",
    "    \"\"\"\n",
    "    word_ndarray = np.array(input_words)\n",
    "    for speaker in good_speakers:\n",
    "        print(speaker)\n",
    "        word_array = np.array(word_ndarray[good_speakers.index(speaker)])\n",
    "        for i in range(10):\n",
    "            index = np.argmax(word_array)\n",
    "            if show_scores:\n",
    "                print(round(word_array[index],2),good_types[index])\n",
    "            else:\n",
    "                print(good_types[index])\n",
    "            word_array[index] = 0\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "create_weighted_word_freq_array_ans",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUCE\n",
      "0.89 genetic\n",
      "0.88 hobby\n",
      "0.86 planners\n",
      "0.83 brake\n",
      "0.83 carpool\n",
      "0.83 presentation\n",
      "0.8 airplanes\n",
      "0.8 fooling\n",
      "0.8 language\n",
      "0.8 mcdonald\n",
      "\n",
      "JONATHAN\n",
      "0.91 erika\n",
      "0.89 katie\n",
      "0.89 pics\n",
      "0.8 awareness\n",
      "0.76 simon\n",
      "0.75 beckham\n",
      "0.75 carmen\n",
      "0.75 command\n",
      "0.75 homely\n",
      "0.75 oats\n",
      "\n",
      "KHLOE\n",
      "0.95 fur\n",
      "0.93 basic\n",
      "0.9 apparently\n",
      "0.89 campaign\n",
      "0.89 secure\n",
      "0.88 begins\n",
      "0.88 fulfilled\n",
      "0.88 furs\n",
      "0.88 moral\n",
      "0.86 creepy\n",
      "\n",
      "KIM\n",
      "0.91 amusing\n",
      "0.89 frizz\n",
      "0.89 song\n",
      "0.86 challenge\n",
      "0.83 hotwire\n",
      "0.83 humiliating\n",
      "0.83 punched\n",
      "0.81 airport\n",
      "0.8 advantage\n",
      "0.8 bruised\n",
      "\n",
      "KOURTNEY\n",
      "0.9 ho\n",
      "0.88 defensive\n",
      "0.83 busted\n",
      "0.83 escalated\n",
      "0.8 absurd\n",
      "0.8 cushion\n",
      "0.8 driveway\n",
      "0.8 prevent\n",
      "0.8 sober\n",
      "0.8 theb\n",
      "\n",
      "KRIS\n",
      "0.9 sweetie\n",
      "0.89 cristal\n",
      "0.88 cranky\n",
      "0.86 angeles\n",
      "0.86 chat\n",
      "0.86 kenneth\n",
      "0.86 los\n",
      "0.86 shops\n",
      "0.83 backup\n",
      "0.83 blew\n",
      "\n",
      "ROBERT\n",
      "0.88 email\n",
      "0.86 anal\n",
      "0.86 truly\n",
      "0.83 adrian\n",
      "0.83 scream\n",
      "0.8 acceptable\n",
      "0.8 overnight\n",
      "0.76 spoken\n",
      "0.75 bashing\n",
      "0.75 cheating\n",
      "\n",
      "SCOTT\n",
      "0.86 sitter\n",
      "0.75 exclamation\n",
      "0.75 gentlemen\n",
      "0.71 hooker\n",
      "0.67 absolute\n",
      "0.67 amigos\n",
      "0.67 assaulted\n",
      "0.67 bateman\n",
      "0.67 blessing\n",
      "0.67 brucinator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top(weighted_words, show_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c3a58ca841fda7e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 11c (Free Response): Specific Word Usage by Character\n",
    "\n",
    "Now we can start to see interesting differences between the characters.\n",
    "\n",
    "Create a new Markdown cell below and use it to write a paragraph discussing the differences you find most striking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't actually watch this show but it's interesting to observe that people does have a tendency in speaking whether consciously or subconsciously. Jonahthan likes to call people's name a lot, I guess? In his top 10 words there are erika, katie, beckham and carmen. Is Scott hispanic? he had amigos in his top 10 and other words that i don't actually understand. Kim says amusing and Khloe says apparently a lot, which is also some words that i'd prefer in real life talking. Anyway, these are just irresponsible assumption but it really says a lot about the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed4cc4acf7ec52c9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This is the end of Assignment 1"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "LangInfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "98ca369906f7b70fe64afc73e530e378539d08b0e41e494e07f1eb5e7a313c26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
