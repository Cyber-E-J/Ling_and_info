{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"executionInfo":{"elapsed":1688,"status":"ok","timestamp":1677180646457,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"ppilw3-9DnDl"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import numpy as np\n","import json\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UAm0MYwYjCZg"},"source":["We'll be using a dataset of movie scripts (this is the same dataset you'll be using in A5!)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":104848,"status":"ok","timestamp":1677180753363,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"xRA6TGOeGDuG","outputId":"3601bb30-7ba4-4d35-f7dd-a328dfc77ccd"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-21dc3c638f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"]}],"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1677183925167,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"Ug2RVauQDnDu","outputId":"2659ee3e-e371-49f2-aa9a-470dc4b55e5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 617 movie transcripts\n","Each movie transcript is a dictionary with the following keys...\n","dict_keys(['movie_name', 'movie_id', 'categories', 'script'])\n","The index of \"spare me\" is 7\n"]}],"source":["with open(\"movie_scripts_data.json\") as f:\n","    data = json.loads(f.readlines()[0])\n","print(\"Loaded {} movie transcripts\".format(len(data)))\n","print(\"Each movie transcript is a dictionary with the following keys...\")\n","print(data[0].keys())\n","\n","# Here, we will assign an index for each movie_id. This index will help us access data in numpy matrices.\n","movie_id_to_index = {movie_id:index for index, movie_id in enumerate([d['movie_id'] for d in data])}\n","\n","# We will also need a dictionary maping movie names to movie ids\n","movie_name_to_id = {name:mid for name, mid in zip([d['movie_name'] for d in data],\n","                                                     [d['movie_id'] for d in data])}\n","movie_id_to_name = {v:k for k,v in movie_name_to_id.items()}\n","\n","# and because it might be useful...\n","movie_name_to_index = {name:movie_id_to_index[movie_name_to_id[name]] for name in [d['movie_name'] for d in data]}\n","movie_index_to_name = {v:k for k,v in movie_name_to_index.items()}\n","\n","print(\"The index of \\\"{}\\\" is {}\".format(data[7]['movie_name'], movie_id_to_index[data[7]['movie_id']]))"]},{"cell_type":"markdown","metadata":{"id":"WNJY0CUgDnDw"},"source":["We can see that each movie is assigned an \"index\" (from 0 to 616). These will correspond to the rows of a document-by-term matrix.\n","\n","Conveniently, the scikit-learn package, which we imported at the top of this notebook, includes a \"CountVectorizer\" class which efficiently implements code to build a document-by-term matrix. It includes many useful settings, including the ability to specify whether we want the matrix elements to be term frequencies (`binary = False`) or boolean/binary (`binary = True`). We will use the latter."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2257,"status":"ok","timestamp":1677184014554,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"sx2pBzfqDnDw"},"outputs":[],"source":["count_vec = CountVectorizer(stop_words = \"english\", max_df = 0.8, min_df = 10, max_features=1000, binary = True)\n","\n","doc_by_vocab = count_vec.fit_transform([x['script'] for x in data])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1677184067721,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"18EAsEUJDnDx","outputId":"d8dc622e-8d82-44d9-c021-bfe632a4db5f"},"outputs":[{"data":{"text/plain":["<617x1000 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 213542 stored elements in Compressed Sparse Row format>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["doc_by_vocab "]},{"cell_type":"markdown","metadata":{"id":"uUsiCzMMklAg"},"source":["Observe that the dimensions are 617 x 1000. The first dimension (rows) exactly equals the number of movie transcripts in our data; this tells us that the format of the matrix is document-by-term (each row is a document, each column is a term)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1677184124042,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"bSO87feADnDx","outputId":"42787e31-eeab-46b2-86c9-57850c8aa5d5","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['able', 'absolutely', 'accept', 'accident', 'account', 'act', 'acting', 'action', 'actually', 'address', 'admit', 'advice', 'afford', 'afraid', 'afternoon', 'age', 'agent', 'ago', 'agree', 'ah', 'ahead', 'ain', 'air', 'alive', 'alright', 'amazing', 'america', 'american', 'angry', 'animal', 'answer', 'answers', 'anybody', 'anymore', 'apart', 'apartment', 'apologize', 'appreciate', 'area', 'aren', 'arm', 'arms', 'army', 'arrest', 'art', 'asked', 'asking', 'asleep', 'ass', 'asshole', 'attack', 'attention', 'awful', 'baby', 'bag', 'ball', 'bank', 'bar', 'bastard', 'bathroom', 'beat', 'beautiful', 'beauty', 'bed', 'beer', 'beg', 'begin', 'beginning', 'belong', 'best', 'bet', 'bigger', 'biggest', 'birthday', 'bit', 'bitch', 'black', 'blame', 'blew', 'blind', 'blood', 'blow', 'blue', 'board', 'boat', 'body', 'book', 'books', 'born', 'boss', 'bother', 'bought', 'bout', 'box', 'boy', 'boys', 'brain', 'brains', 'break', 'breakfast', 'breath', 'bring', 'bringing', 'broke', 'broken', 'brother', 'brothers', 'brought', 'bucks', 'buddy', 'build', 'building', 'built', 'bullshit', 'bunch', 'burn', 'bus', 'business', 'busy', 'buy', 'bye', 'called', 'calling', 'calls', 'calm', 'captain', 'car', 'card', 'care', 'career', 'careful', 'cares', 'carry', 'case', 'cash', 'cat', 'catch', 'caught', 'cause', 'certain', 'certainly', 'chance', 'change', 'changed', 'charge', 'cheap', 'check', 'checked', 'chief', 'child', 'children', 'choice', 'christ', 'christmas', 'church', 'city', 'class', 'clean', 'clear', 'clock', 'close', 'closed', 'closer', 'clothes', 'club', 'coffee', 'cold', 'college', 'color', 'comes', 'comfortable', 'common', 'company', 'complete', 'completely', 'computer', 'concerned', 'consider', 'contact', 'control', 'conversation', 'cool', 'cop', 'cops', 'copy', 'corner', 'correct', 'cost', 'couldn', 'count', 'country', 'couple', 'court', 'cover', 'crap', 'crazy', 'credit', 'crime', 'cross', 'crying', 'curious', 'cut', 'cute', 'dad', 'daddy', 'damn', 'dance', 'danger', 'dangerous', 'dark', 'darling', 'date', 'daughter', 'days', 'deal', 'dear', 'death', 'decide', 'decided', 'decision', 'deep', 'definitely', 'department', 'depends', 'deserve', 'desk', 'destroy', 'dick', 'die', 'died', 'difference', 'different', 'difficult', 'dig', 'dinner', 'dirty', 'discuss', 'doctor', 'dog', 'doin', 'dollar', 'dollars', 'door', 'double', 'doubt', 'dr', 'dream', 'dreams', 'dress', 'dressed', 'drink', 'drinking', 'drive', 'driving', 'drop', 'dropped', 'drugs', 'drunk', 'dry', 'dumb', 'dying', 'early', 'earth', 'easier', 'east', 'easy', 'eat', 'eating', 'eh', 'em', 'end', 'english', 'enjoy', 'entire', 'especially', 'evening', 'everybody', 'evidence', 'evil', 'exactly', 'excited', 'excuse', 'expect', 'expecting', 'experience', 'explain', 'extra', 'eye', 'eyes', 'face', 'fact', 'fair', 'fall', 'family', 'far', 'fast', 'fat', 'father', 'fault', 'favor', 'favorite', 'fear', 'feed', 'feeling', 'feelings', 'feels', 'feet', 'fell', 'fellow', 'felt', 'field', 'fight', 'fighting', 'figure', 'figured', 'finally', 'fine', 'finish', 'finished', 'fired', 'fish', 'fit', 'fix', 'floor', 'fly', 'folks', 'follow', 'followed', 'following', 'food', 'fool', 'foot', 'force', 'forever', 'forget', 'forgive', 'forgot', 'form', 'forward', 'free', 'french', 'friend', 'friends', 'fuck', 'fucked', 'fuckin', 'fucking', 'fun', 'funny', 'future', 'game', 'games', 'gas', 'gave', 'general', 'gentlemen', 'gets', 'gift', 'gimme', 'girl', 'girlfriend', 'girls', 'given', 'gives', 'giving', 'glad', 'glass', 'goddamn', 'goes', 'goin', 'gold', 'gone', 'gonna', 'goodbye', 'gotta', 'gotten', 'government', 'grab', 'grand', 'greatest', 'green', 'ground', 'group', 'grow', 'guard', 'guess', 'gun', 'guns', 'guy', 'guys', 'hadn', 'hair', 'half', 'hall', 'hand', 'handle', 'hands', 'hang', 'hanging', 'happen', 'happening', 'happens', 'happy', 'hard', 'hardly', 'hasn', 'hate', 'haven', 'having', 'head', 'heads', 'heard', 'hearing', 'heart', 'heaven', 'heavy', 'held', 'hell', 'hello', 'helped', 'hey', 'hi', 'hide', 'hiding', 'high', 'history', 'hit', 'hold', 'holding', 'hole', 'holy', 'honest', 'honey', 'honor', 'hope', 'hoping', 'horrible', 'horse', 'hospital', 'hot', 'hotel', 'hour', 'hours', 'house', 'huh', 'human', 'hungry', 'hurry', 'hurt', 'husband', 'ice', 'idea', 'ideas', 'idiot', 'imagine', 'immediately', 'important', 'impossible', 'information', 'innocent', 'insane', 'inside', 'instead', 'interested', 'interesting', 'involved', 'island', 'jack', 'jail', 'jesus', 'job', 'john', 'join', 'joke', 'judge', 'jump', 'keeping', 'keeps', 'kept', 'key', 'kick', 'kid', 'kidding', 'kids', 'kill', 'killed', 'killer', 'killing', 'kinda', 'king', 'kiss', 'kitchen', 'knew', 'knock', 'knowing', 'known', 'knows', 'ladies', 'lady', 'laid', 'land', 'large', 'late', 'lately', 'later', 'laugh', 'law', 'lawyer', 'lay', 'lead', 'learn', 'learned', 'leaves', 'leaving', 'leg', 'legs', 'letter', 'letting', 'level', 'liar', 'lie', 'lied', 'light', 'lights', 'liked', 'likes', 'line', 'list', 'listening', 'live', 'lived', 'lives', 'living', 'lock', 'locked', 'lonely', 'longer', 'looked', 'looks', 'loose', 'lord', 'lose', 'losing', 'lost', 'lots', 'loved', 'lovely', 'loves', 'low', 'luck', 'lucky', 'lunch', 'lying', 'ma', 'machine', 'mad', 'main', 'major', 'makes', 'making', 'mark', 'marriage', 'married', 'marry', 'matter', 'matters', 'meaning', 'means', 'meant', 'meet', 'meeting', 'memory', 'men', 'mention', 'mess', 'message', 'met', 'middle', 'miles', 'million', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mission', 'mistake', 'mister', 'mom', 'moment', 'mon', 'money', 'month', 'months', 'morning', 'mother', 'mouth', 'moved', 'movie', 'movies', 'moving', 'mr', 'mrs', 'murder', 'music', 'named', 'names', 'natural', 'nature', 'near', 'necessary', 'neck', 'needed', 'needs', 'nervous', 'news', 'nice', 'nope', 'normal', 'north', 'nose', 'note', 'nothin', 'notice', 'noticed', 'number', 'numbers', 'nuts', 'obviously', 'offer', 'office', 'officer', 'okay', 'older', 'ones', 'open', 'opened', 'opinion', 'order', 'orders', 'ought', 'outside', 'outta', 'owe', 'pack', 'paid', 'pain', 'pal', 'paper', 'papers', 'pardon', 'parents', 'park', 'partner', 'party', 'pass', 'passed', 'past', 'pay', 'paying', 'peace', 'percent', 'perfect', 'perfectly', 'person', 'personal', 'personally', 'phone', 'pick', 'picked', 'picture', 'pictures', 'piece', 'pieces', 'places', 'plan', 'plane', 'plans', 'play', 'played', 'playing', 'pleasure', 'plenty', 'plus', 'point', 'police', 'poor', 'position', 'possible', 'possibly', 'power', 'present', 'president', 'press', 'pretty', 'price', 'prison', 'private', 'probably', 'problem', 'problems', 'promise', 'promised', 'protect', 'proud', 'prove', 'public', 'pull', 'pulled', 'push', 'putting', 'question', 'questions', 'quick', 'quickly', 'quiet', 'quit', 'quite', 'radio', 'raise', 'ran', 'reach', 'read', 'reading', 'ready', 'realize', 'reason', 'recognize', 'record', 'records', 'red', 'relationship', 'relax', 'report', 'respect', 'responsible', 'rest', 'return', 'rich', 'rid', 'ride', 'ridiculous', 'ring', 'risk', 'road', 'rock', 'room', 'rough', 'round', 'rules', 'run', 'running', 'runs', 'sad', 'safe', 'sake', 'save', 'saved', 'saw', 'saying', 'says', 'scare', 'scared', 'school', 'screw', 'sea', 'seat', 'second', 'seconds', 'secret', 'security', 'seeing', 'seen', 'sees', 'self', 'sell', 'send', 'sense', 'sent', 'seriously', 'service', 'set', 'seven', 'sex', 'shall', 'share', 'ship', 'shit', 'shoes', 'shoot', 'short', 'shot', 'shouldn', 'showed', 'shows', 'shut', 'sick', 'sight', 'sign', 'silly', 'simple', 'single', 'sir', 'sister', 'sit', 'sitting', 'situation', 'size', 'skin', 'sleep', 'sleeping', 'slow', 'small', 'smart', 'smell', 'smoke', 'sold', 'somebody', 'son', 'song', 'soon', 'sooner', 'sort', 'soul', 'sound', 'sounds', 'south', 'space', 'speak', 'speaking', 'special', 'spend', 'spent', 'split', 'spoke', 'spot', 'stand', 'standing', 'star', 'start', 'started', 'starting', 'starts', 'state', 'station', 'staying', 'steal', 'step', 'stick', 'stole', 'stopped', 'store', 'stories', 'story', 'straight', 'strange', 'street', 'strong', 'stuck', 'study', 'stuff', 'stupid', 'subject', 'suddenly', 'suit', 'summer', 'sun', 'suppose', 'supposed', 'surprise', 'surprised', 'swear', 'sweet', 'table', 'taken', 'takes', 'taking', 'talked', 'tape', 'taste', 'taught', 'teach', 'team', 'teeth', 'telling', 'tells', 'terrible', 'test', 'thank', 'thanks', 'thinking', 'thinks', 'thirty', 'thousand', 'throw', 'ticket', 'tie', 'tight', 'till', 'times', 'tired', 'today', 'tomorrow', 'tonight', 'took', 'totally', 'touch', 'tough', 'town', 'track', 'train', 'treat', 'tree', 'trick', 'tried', 'trip', 'trouble', 'truck', 'true', 'trust', 'truth', 'trying', 'turn', 'turned', 'turning', 'turns', 'tv', 'twice', 'type', 'uh', 'um', 'uncle', 'unless', 'upset', 'upstairs', 'use', 'using', 'usual', 'usually', 'view', 'visit', 'voice', 'waiting', 'wake', 'walk', 'walked', 'walking', 'wall', 'wanna', 'wants', 'war', 'warm', 'waste', 'watch', 'watching', 'water', 'ways', 'wear', 'wearing', 'week', 'weekend', 'weeks', 'weird', 'welcome', 'went', 'weren', 'west', 'white', 'wife', 'wild', 'willing', 'win', 'wind', 'window', 'wish', 'woman', 'women', 'wonder', 'wonderful', 'wondering', 'word', 'words', 'worked', 'working', 'works', 'world', 'worried', 'worry', 'worse', 'worst', 'worth', 'wow', 'write', 'writing', 'wrote', 'ya', 'year', 'yesterday', 'york', 'young']\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["features = count_vec.get_feature_names()\n","print(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r0OIKSLDnDy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"executionInfo":{"elapsed":420,"status":"ok","timestamp":1677184144094,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"sAyJ6kgPDnDy"},"outputs":[],"source":["doc_by_vocab = doc_by_vocab.toarray()\n"]},{"cell_type":"markdown","metadata":{"id":"muEBBdDKlLh3"},"source":["Right now we have a document-by-term matrix. For deriving a co-occurrence matrix, it is more natural to start from a term-by-document matrix (such that each row is a term), which we can easily obtain by taking the transpose."]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":true,"executionInfo":{"elapsed":167,"status":"ok","timestamp":1677184163298,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"mTnVbs7ODnDy"},"outputs":[],"source":["term_document_matrix = doc_by_vocab.T"]},{"cell_type":"markdown","metadata":{"id":"kCcdIAY4lZ7g"},"source":["Then, the co-occurrence matrix can be derived by taking the dot product between every pair of terms, which can be efficiently done via... [?]"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"executionInfo":{"elapsed":1679,"status":"ok","timestamp":1677184437416,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"FUaSH3v_DnDz"},"outputs":[],"source":["cooccurence_matrix = np.dot(term_document_matrix, doc_by_vocab)"]},{"cell_type":"markdown","metadata":{"id":"1_-8r-o2lo19"},"source":["Let's try using this to compute similarity between words!"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":true,"executionInfo":{"elapsed":195,"status":"ok","timestamp":1677184510604,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"idr6KBEADnDz"},"outputs":[],"source":["def find_most_similar_words(word, sim_matrix = cooccurence_matrix, topk=100):\n","    if word not in features:\n","        print(word, 'is OOV.')\n","        return None \n","    idx = features.index(word)\n","    sorted_words = np.argsort(sim_matrix[idx])[::-1]\n","    print('Most similar {} words to \"{}\" are:'.format(topk, word))\n","    for i in range(topk):\n","        j = sorted_words[i]\n","        print(features[j], sim_matrix[idx, j])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332,"status":"ok","timestamp":1677184519526,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"K9DfAennDnDz","outputId":"785e085b-722a-4a27-9de9-62050fd04d20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most similar 10 words to \"class\" are:\n","class 171\n","best 155\n","trying 154\n","care 149\n","guess 148\n","thank 148\n","fine 148\n","went 148\n","seen 147\n","matter 147\n"]}],"source":["find_most_similar_words(\"class\", sim_matrix = cooccurence_matrix , topk = 10)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391,"status":"ok","timestamp":1677184624032,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"nn2DaPJgDnDz","outputId":"d9a7b3e1-2b25-4066-9d97-5684bb2157ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most similar 5 words to \"guess\" are:\n","guess 474\n","okay 399\n","trying 395\n","care 395\n","hey 391\n"]}],"source":["find_most_similar_words(\"guess\", sim_matrix = cooccurence_matrix , topk = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4obvmW56DnD0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"OkPh2WPDl5Rp"},"source":["Let's efficiently compute PMI, which for *ranking* purposes can be simplified to n_ab / (n_a * n_b). Our co-occurrence matrix already contains the numerator (co-occurrence counts of all term pairs). Both terms in the denominator can be obtained from the document frequency (DF) vector, which we can easily compute from our term-document matrix."]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":true,"executionInfo":{"elapsed":240,"status":"ok","timestamp":1677185742297,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"HFiyHH2zDnD0"},"outputs":[],"source":["df = np.sum(term_document_matrix,1)"]},{"cell_type":"markdown","metadata":{"id":"kcw6rws2mZbS"},"source":["Dividing each row by the DF vector (which is easy to express in numpy) does the division by n_b..."]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":true,"executionInfo":{"elapsed":12,"status":"ok","timestamp":1677185776367,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"HGehLwXqDnD0","scrolled":false},"outputs":[],"source":["PMI_part = cooccurence_matrix / df"]},{"cell_type":"markdown","metadata":{"id":"YAvPl4SEmi96"},"source":["...then we just need to divide again over each column (which is also easy to express in numpy, as long as we turn the DF vector into a column-shaped vector) to do the division by n_a."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1677185779682,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"-5eYpDjNDnD0"},"outputs":[],"source":["PMI = PMI_part/df.reshape(df.shape[0],1)"]},{"cell_type":"markdown","metadata":{"id":"fl5bkobFmxkm"},"source":["Let's see how the similar words changed!"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210,"status":"ok","timestamp":1677185793076,"user":{"displayName":"Jonathan Chang","userId":"10588004462392993302"},"user_tz":300},"id":"Yhs7zvSSDnD0","outputId":"fe58544d-7b74-4fed-c1dd-81d2c2ac8acd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most similar 5 words to \"class\" are:\n","class 0.005847953216374269\n","weekend 0.002789849240839101\n","biggest 0.0026990553306342783\n","rules 0.002549107812265707\n","board 0.002519849759901108\n"]}],"source":["find_most_similar_words(\"class\", sim_matrix = PMI , topk = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"JIbHIkXJDnD1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SX_D7b1IDnD1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"6zVNW-NRDnD1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"FAJvRot0DnD1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Xwi4e4DqDnD1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"sG6GQmJADnD1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"o1M5GHb0DnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ATT0ujb8DnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"dY-4lEdxDnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nP0PYvKpDnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"GWQfMdaNDnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WzkkmCMPDnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"dsqqMaCJDnD2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaaZKjFPDnD3"},"outputs":[],"source":["x = np.array([[1,2,3], [4,5,6], [7,8,9]])\n","v = np.array([1, -1, -2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQl01DJpnGoP"},"outputs":[],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oz_D9DVcnL4x"},"outputs":[],"source":["x/v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xY_w9UUJnP5"},"outputs":[],"source":["v.reshape(3,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNOmCLulDnD3"},"outputs":[],"source":["x/v.reshape(3,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"T8Ow9oseDnD3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"fxPehbwNDnD3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"h2hXOTIiDnD3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IznCpT7vDnD3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Tf0uJBxVDnD4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0oiwx1YGDnD4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PZQ3qTXmDnD4"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[{"file_id":"1XBoulAqDIZ0npouOuub_Qwkw685WrZDi","timestamp":1677177367067},{"file_id":"1SbHjBebn0-hcPLxZUFYmnBawMLIKx7ht","timestamp":1616085861431}]},"kernelspec":{"display_name":"LangInfo","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
